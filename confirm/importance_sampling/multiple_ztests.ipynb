{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore importance sampling on some trivial test cases.\n",
    "\n",
    "The steps of importance sampling are described in the overleaf, Appendix D.\n",
    "\n",
    "We will implement them first for the 1-dimensional z-test, then the 2- and 3-dimensional z-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas \n",
    "import seaborn \n",
    "import sklearn \n",
    "#import ipython\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.linspace(-2, 2, 11)\n",
    "z = np.random.normal(0, 1, 1000)\n",
    "data = mu[None,:] + z[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data =  data.flatten()\n",
    "#Now, we select the rejections in order to run k-means on them\n",
    "selection = flat_data > 1.96\n",
    "rejections = flat_data[selection]\n",
    "standardized_rejections = (rejections - np.mean(rejections))/np.std(rejections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans= KMeans(n_clusters=n_clusters,\n",
    "               init = \"random\",\n",
    "            n_init=10,\n",
    "               max_iter=300,\n",
    "               random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=5, n_init=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=5, n_init=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(init='random', n_clusters=5, n_init=10, random_state=42)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(standardized_rejections.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.97053085],\n",
       "       [2.46820867],\n",
       "       [3.31656385],\n",
       "       [2.11366402],\n",
       "       [2.87094621]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_cluster_centers = kmeans.cluster_centers_ * np.std(rejections) + np.mean(rejections)\n",
    "mu_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 1, ..., 1, 4, 3], dtype=int32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1,  2,  3,  4], dtype=int32),\n",
       " array([9602,   83,  383,  188,  459,  285]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_orig = len(flat_data)\n",
    "flat_labels = np.full(n_orig, -1,dtype = np.int32)\n",
    "flat_labels[selection] = kmeans.labels_\n",
    "np.unique(flat_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = flat_labels.reshape(data.shape)\n",
    "n_sims_per_theta = data.shape[0] # 1000 for now\n",
    "n_theta = data.shape[1] # 11 for now\n",
    "target_fraction = np.full((n_clusters + 1, n_theta),-1)\n",
    "labelset = np.unique(labels)\n",
    "labelbins = np.append(labelset - 0.5,n_clusters - 0.5)\n",
    "for i in range(n_theta):\n",
    "    target_fraction[:,i] = np.histogram(labels[:,i], bins = labelbins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1000, 1000, 1000,  999,  994,  971,  938,  865,  743,  630,  462],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    1,    4,   27,   51],\n",
       "       [   0,    0,    0,    0,    2,    6,   24,   46,   70,  113,  122],\n",
       "       [   0,    0,    0,    0,    0,    0,    2,    7,   28,   51,  100],\n",
       "       [   0,    0,    0,    1,    4,   21,   30,   57,  106,   99,  141],\n",
       "       [   0,    0,    0,    0,    0,    2,    6,   24,   49,   80,  124]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_fraction\n",
    "# looks good so far!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get into the business of doing the importance samples and re-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -1.6       , -1.2       , -0.8       , -0.4       ,\n",
       "        0.        ,  0.4       ,  0.8       ,  1.2       ,  1.6       ,\n",
       "        2.        ,  3.97053085,  2.46820867,  3.31656385,  2.11366402,\n",
       "        2.87094621])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we construct the weights matrix: how many sims are we planning for each value of theta?\n",
    "n_per_thetaj = 1000\n",
    "# We want this to net out to, let's say, 1% of the total weight...\n",
    "sum_rejects = np.delete(target_fraction, 0, axis = 0)\n",
    "any_successes = np.sum(sum_rejects, axis = 0) > 0\n",
    "any_successes\n",
    "relevant_mu = mu[any_successes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/5vk3q66x569bvnl4f7b0q91r0000gn/T/ipykernel_8896/3285810566.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  temp = sum_rejects / np.sum(sum_rejects,axis = 0)[None,:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 8)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sum_rejects / np.sum(sum_rejects,axis = 0)[None,:]\n",
    "wjj = 0.01\n",
    "important_weights = temp[:,any_successes]*(1 - wjj)\n",
    "important_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,), (5, 8)]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[relevant_mu.shape , important_weights.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(np.full_like(relevant_mu,wjj)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_weights = np.append(np.diag(np.full_like(relevant_mu,wjj)), important_weights, axis = 0)\n",
    "np.sum(full_weights, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the importance samples:\n",
    "mu_importance = np.append(relevant_mu,mu_cluster_centers)\n",
    "z_importance = np.random.normal(0, 1, size = 1000 * len(mu_importance)).reshape(1000, len(mu_importance))\n",
    "data_importance = mu_importance[None,:] + z_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 13), (13,), (8,), (8, 13))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_importance.shape, mu_importance.shape, relevant_mu.shape, np.transpose(full_weights).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions: i (simulations), j (initial theta), k (importance theta which generates samples), m (second dummy copy of importance theta)\n",
    "inside_exponent = data_importance[:,None,:, None]*(mu_importance[None,None,None, :] - relevant_mu[None,:,None, None]) - mu_importance[None,None,None,:]**2/2 + relevant_mu[None,:,None, None]**2/2\n",
    "likelihood_ratios = np.exp(inside_exponent) # I bet this is the problem! Look in this line and the above for a bug!\n",
    "denoms = np.sum(likelihood_ratios * np.transpose(full_weights)[None, :, None, :], axis = 3)\n",
    "rejects = data_importance > 1.96\n",
    "inner_mean = np.mean(rejects[:,None,:]/denoms, axis = 0) #this is the inner sum divided by n_j\n",
    "inner_mse_estimate = np.mean((rejects[:,None,:]/denoms)**2, axis = 0) - inner_mean**2 # trying to do an empirical calculation of the variance of each obs\n",
    "final_result = np.sum(inner_mean * np.transpose(full_weights), axis = 1)\n",
    "final_variance_estimate =np.sum((inner_mse_estimate/1000) * (np.transpose(full_weights)**2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00279485, 0.00905081, 0.02472714, 0.05938342, 0.12325566,\n",
       "       0.22408063, 0.36091008, 0.51831282])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.44363075e-08, 1.23098518e-07, 8.11344287e-07, 2.78396900e-06,\n",
       "       8.33830844e-06, 1.97106349e-05, 3.27258136e-05, 4.35984712e-05])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_variance_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([114.05337211,  72.85946643,  29.72315337,  20.06381023,\n",
       "        12.95990717,   8.82105016,   7.04807515,   5.72645401])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estimated sample size ratio\n",
    "((final_result * (1-final_result)) / (1000)) /final_variance_estimate\n",
    "# Hmm... this is not excellent. Importance sampling is doing better for the first few values of mu, but not consistently for larger values.\n",
    "# I'm betting that this is due to an incorrect variance calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8       , -0.4       ,  0.        ,  0.4       ,  0.8       ,\n",
       "        1.2       ,  1.6       ,  2.        ,  3.97053085,  2.46820867,\n",
       "        3.31656385,  2.11366402,  2.87094621])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator formula:\n",
    "\n",
    "denom = sum w_jk Pk/Pj (X).\n",
    "\n",
    "The likelihood ratio is exp([x -\\ mu_j]^2/2 - [x-\\mu_k]^2/2) = exp(-mu_k^2/2 + mu_j^2/2 + x(mu_k - mu_j))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generalize this to two-dimensional mu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.linspace(-2, 2, 11)\n",
    "z = np.random.normal(0, 1, 1000)\n",
    "data = mu[None,:] + z[:,None]\n",
    "flat_data =  data.flatten()\n",
    "#Now, we select the rejections in order to run k-means on them\n",
    "selection = flat_data > 1.96\n",
    "rejections = flat_data[selection]\n",
    "standardized_rejections = (rejections - np.mean(rejections))/np.std(rejections)\n",
    "n_clusters = 5\n",
    "kmeans= KMeans(n_clusters=n_clusters,\n",
    "               init = \"random\",\n",
    "            n_init=10,\n",
    "               max_iter=300,\n",
    "               random_state = 42)\n",
    "kmeans.fit(standardized_rejections.reshape(-1,1))\n",
    "mu_cluster_centers = kmeans.cluster_centers_ * np.std(rejections) + np.mean(rejections)\n",
    "kmeans.labels_\n",
    "n_orig = len(flat_data)\n",
    "flat_labels = np.full(n_orig, -1,dtype = np.int32)\n",
    "flat_labels[selection] = kmeans.labels_\n",
    "labels = flat_labels.reshape(data.shape)\n",
    "n_sims_per_theta = data.shape[0] # 1000 for now\n",
    "n_theta = data.shape[1] # 11 for now\n",
    "target_fraction = np.full((n_clusters + 1, n_theta),-1)\n",
    "labelset = np.unique(labels)\n",
    "labelbins = np.append(labelset - 0.5,n_clusters - 0.5)\n",
    "for i in range(n_theta):\n",
    "    target_fraction[:,i] = np.histogram(labels[:,i], bins = labelbins)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pilot sims done, now the real run:\n",
    "n_per_thetaj = 1000\n",
    "# We want this to net out to, let's say, 1% of the total weight...\n",
    "sum_rejects = np.delete(target_fraction, 0, axis = 0)\n",
    "any_successes = np.sum(sum_rejects, axis = 0) > 0\n",
    "any_successes\n",
    "relevant_mu = mu[any_successes]\n",
    "temp = sum_rejects / np.sum(sum_rejects,axis = 0)[None,:]\n",
    "wjj = 0.01\n",
    "important_weights = temp[:,any_successes]*(1 - wjj)\n",
    "full_weights = np.append(np.diag(np.full_like(relevant_mu,wjj)), important_weights, axis = 0)\n",
    "# Now we do the importance samples:\n",
    "mu_importance = np.append(relevant_mu,mu_cluster_centers)\n",
    "z_importance = np.random.normal(0, 1, size = 1000 * len(mu_importance)).reshape(1000, len(mu_importance))\n",
    "data_importance = mu_importance[None,:] + z_importance\n",
    "inside_exponent = data_importance[:,None,:, None]*(mu_importance[None,None,None, :] - relevant_mu[None,:,None, None]) - mu_importance[None,None,None,:]**2/2 + relevant_mu[None,:,None, None]**2/2\n",
    "likelihood_ratios = np.exp(inside_exponent) # I bet this is the problem! Look in this line and the above for a bug!\n",
    "denoms = np.sum(likelihood_ratios * np.transpose(full_weights)[None, :, None, :], axis = 3)\n",
    "rejects = data_importance > 1.96\n",
    "inner_mean = np.mean(rejects[:,None,:]/denoms, axis = 0) #this is the inner sum divided by n_j\n",
    "inner_mse_estimate = np.mean((rejects[:,None,:]/denoms)**2, axis = 0) - inner_mean**2 # trying to do an empirical calculation of the variance of each obs\n",
    "final_result = np.sum(inner_mean * np.transpose(full_weights), axis = 1)\n",
    "final_variance_estimate =np.sum((inner_mse_estimate/1000) * (np.transpose(full_weights)**2), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confirm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244a70f8ba587ac945a75283a2462feda5931af2453571227e17a5a68b0d3888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
