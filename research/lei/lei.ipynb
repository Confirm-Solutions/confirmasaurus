{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlaw\n",
    "import outlaw.berry as berry\n",
    "import outlaw.quad as quad\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import time\n",
    "import outlaw.inla as inla\n",
    "import matplotlib.pyplot as plt\n",
    "import numpyro.distributions as dist\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "\n",
    "from lewis import lewis\n",
    "from lewis import batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lei Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following description is a clinical trial design using a Bayesian model with early-stopping rules for futility or efficacy of a drug.\n",
    "This design was explicitly requested to be studied by an FDA member (Lei) in the CID team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following is a randomized, double-blind, placebo-controlled two-stage adaptive design intended to identify an optimal treatment regimen \n",
    "> from three possible regimens (for example, different dosages or different combinations of agents) and \n",
    "> to assess the efficacy of that regimen with respect to a primary binary response endpoint measured at month 6.\n",
    "> \n",
    "> In Stage 1, one of four experimental regimens will be selected, or the trial will stop for futility. \n",
    "> In this stage, a minimum of 200 and a maximum of 400 will be randomized 1:1:1:1 to one of the three experimental arms or one placebo arm. \n",
    "> Interim analyses will be conducted after 200, 300 and 400 subjects have been enrolled to select the best experimental regimen and to potentially stop \n",
    "> the trial for futility. \n",
    "> If an experimental regimen is dropped for futility at an interim analysis, \n",
    "> the next 100 subjects to be randomized will be allocated equally among the remaining arms in the study. \n",
    "> At each of these three analysis time points (N = 200, 300, 400), \n",
    "> the probabilities of being the best regimen (PrBest) and predictive probability of success (PPS) \n",
    "> are calculated for each experimental regimen using a Bayesian approach, \n",
    "> and the trial will either stop for futility, \n",
    "> continue to the next interim analysis, \n",
    "> or proceed to Stage 2 depending on the results of these PrBest and PPS calculations.\n",
    "> \n",
    "> In Stage 2, a minimum of 200 and a maximum of 400 additional subjects will be randomized 1:1 to the chosen regimen or placebo. \n",
    "> The two groups (pooling both Stage 1 and Stage 2 subjects) will be compared for efficacy and futility assessment at an interim analysis \n",
    "> after 200 subjects have been enrolled in Stage 2, \n",
    "> and for efficacy at a final analysis after 400 subjects have been enrolled in Stage 2 and fully followed-up for response. \n",
    "> The study may be stopped for futility or efficacy based on PPS at the interim analysis. \n",
    "> If the study continues to the final analysis, \n",
    "> the posterior distribution of the difference in response rates between placebo and the chosen experimental arm \n",
    "> will be evaluated against a pre-specified decision criterion.\n",
    "> \n",
    "> - Scenario 1: interim analyses are based on available data on the primary endpoint (measured at month 6)\n",
    "> - Scenario 2: interim analyses are based on available data on a secondary endpoint (measured at month 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook breaks down and discusses the components of the trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y \\in \\mathbb{N}^d$: Binomial responses.\n",
    "- $p \\in [0,1]^d$: probability parameter to the Binomial distribution.\n",
    "- $n \\in \\mathbb{N}^d$: size parameter to the Binomial distribution.\n",
    "- $q \\in [0,1]^d$: base probability value to offset $p$.\n",
    "- $\\theta \\in \\R^d$: logit parameter that determines $p$.\n",
    "- $\\mu \\in \\mathbb{R}$: shared mean parameter among $\\theta_i$.\n",
    "- $\\sigma^2 \\in \\mathbb{R}_+$: shared variance parameter among $\\theta_i$.\n",
    "- $\\mu_0, \\sigma_0^2, \\alpha_0, \\beta_0 \\in \\mathbb{R}$: hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian model is described below:\n",
    "\\begin{align*}\n",
    "y_i | p_i &\\sim \\mathrm{Binom}(n_i, p_i) \\quad i = 1,\\ldots, d \\\\\n",
    "p_i &= {\\sf expit}(\\theta_i + \\mathrm{logit}(q_i) ) \\quad i = 1,\\ldots, d \\\\\n",
    "\\theta | \\mu, \\sigma^2 &\\sim \\mathcal{N}(\\mu \\mathbb{1}, \\sigma^2 I) \\\\\n",
    "\\mu &\\sim \\mathcal{N}(\\mu_0, \\sigma_0^2) \\\\\n",
    "\\sigma^2 &\\sim \\Gamma^{-1}(\\alpha_0, \\beta_0) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We note in passing that the model can be collapsed along $\\mu$ to get:\n",
    "\\begin{align*}\n",
    "y_i | p_i &\\sim \\mathrm{Binom}(n_i, p_i) \\quad i = 1,\\ldots, d \\\\\n",
    "p_i &= {\\sf expit}(\\theta_i + \\mathrm{logit}(q_i) ) \\quad i = 1,\\ldots, d \\\\\n",
    "\\theta | \\sigma^2 &\\sim \\mathcal{N}(\\mu_0 \\mathbb{1}, \\sigma^2 I + \\sigma_0^2 \\mathbb{1} \\mathbb{1}^\\top) \\\\\n",
    "\\sigma^2 &\\sim \\Gamma^{-1}(\\alpha_0, \\beta_0) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probability of Best Arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first quantity of interest is probability of best (treatment) arm.\n",
    "Concretely, letting $i = 1$ denote the control arm, we wish to compute for each $1 < i \\leq d$:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(p_i > \\max\\limits_{j \\neq i} p_j | y, n)\n",
    "&=\n",
    "\\int \\mathbb{P}(p_i > \\max\\limits_{j \\neq i} p_j | y, n, \\sigma^2) p(\\sigma^2 | y, n) \\, d\\sigma^2\n",
    "\\\\&=\n",
    "\\int \\mathbb{P}(\\theta_i + c_i > \\max\\limits_{j \\neq i} (\\theta_j + c_j) | y, n, \\sigma^2) p(\\sigma^2 | y, n) \\, d\\sigma^2\n",
    "\\end{align*}\n",
    "where $c = \\mathrm{logit}(q)$.\n",
    "We can approximate this quantity by estimating the two integrand terms separately. \n",
    "By approximating $\\theta_i | y, n$ as normal, the first integrand term can be estimated by Monte Carlo.\n",
    "The second term can be estimated by INLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_normal_best(mean, cov, key, n_sims):\n",
    "    '''\n",
    "    Estimates P[X_i > max_{j != i} X_j] where X ~ N(mean, cov) via sampling.\n",
    "    '''\n",
    "    out_shape = (n_sims, *mean.shape[:-1])\n",
    "    sims = jax.random.multivariate_normal(key, mean, cov, shape=out_shape)\n",
    "    order = jnp.arange(1, mean.shape[-1])\n",
    "    compute_pr_best_all = jax.vmap(lambda i: jnp.mean(jnp.argmax(sims, axis=-1) == i, axis=0))\n",
    "    return compute_pr_best_all(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "mean = jnp.array([2, 2, 2, 5])\n",
    "cov = jnp.eye(d)\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_sims = 100000\n",
    "jax.jit(pr_normal_best, static_argnums=(3,))(mean, cov, key, n_sims=n_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform INLA to estimate $p(\\sigma^2 | y, n)$ on a grid of values for $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig2_rule = quad.log_gauss_rule(15, 1e-6, 1e3)\n",
    "sig2_rule_ops = berry.optimized(sig2_rule.pts, n_arms=4).config(\n",
    "    opt_tol=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_sigma_sq(data, sig2_rule, sig2_rule_ops):\n",
    "    n_arms, _ = data.shape\n",
    "    sig2 = sig2_rule.pts\n",
    "    n_sig2 = sig2.shape[0]\n",
    "    p_pinned = dict(sig2=sig2, theta=None)\n",
    "\n",
    "    f = sig2_rule_ops.laplace_logpost\n",
    "    logpost, x_max, hess, iters = f(\n",
    "        np.zeros((n_sig2, n_arms)), p_pinned, data\n",
    "    )\n",
    "    post = inla.exp_and_normalize(\n",
    "            logpost, sig2_rule.wts, axis=-1)\n",
    "\n",
    "    return post, x_max, hess, iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = jnp.float64\n",
    "N = 1\n",
    "data = berry.figure2_data(N).astype(dtype)[0]\n",
    "n_arms, _ = data.shape\n",
    "posterior_sigma_sq_jit = jax.jit(lambda data: posterior_sigma_sq(data, sig2_rule, sig2_rule_ops))\n",
    "post, _, hess, _ = posterior_sigma_sq_jit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the two pieces together, we have the following function to compute the probability of best treatment arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_best(data, sig2_rule, sig2_rule_ops, key, n_sims):\n",
    "    n_arms, _ = data.shape\n",
    "    post, x_max, hess, _ = posterior_sigma_sq(data, sig2_rule, sig2_rule_ops) \n",
    "    mean = x_max\n",
    "    hess_fn = jax.vmap(lambda h: jnp.diag(h[0]) + jnp.full(shape=(n_arms, n_arms), fill_value=h[1]))\n",
    "    prec = -hess_fn(hess) # (n_sigs, n_arms, n_arms)\n",
    "    cov = jnp.linalg.inv(prec)\n",
    "    pr_normal_best_out = pr_normal_best(mean, cov, key=key, n_sims=n_sims)\n",
    "    return jnp.matmul(pr_normal_best_out, post * sig2_rule.wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 13\n",
    "out = pr_best(data, sig2_rule, sig2_rule_ops, key, n_sims)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase III Final Analysis\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\theta_i - \\theta_0 < t | y, n) < 0.1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\theta_i - \\theta_0 < t | y, n)\n",
    "&=\n",
    "\\mathbb{P}(q_1^\\top \\theta < t | y,n)\n",
    "=\n",
    "\\int \\mathbb{P}(q_1^\\top \\theta < t | y, n, \\sigma^2) p(\\sigma^2 | y, n) \\, d\\sigma^2\n",
    "\\\\&=\n",
    "\\int \\mathbb{P}(q_1^\\top \\theta < t | y, n, \\sigma^2) p(\\sigma^2 | y, n) \\, d\\sigma^2\n",
    "\\\\\n",
    "q_1^\\top \\theta | y, n, \\sigma^2 &\\sim \\mathcal{N}(q_1^\\top \\theta^*, -q_1^\\top (H\\log p(\\theta^*, y, \\sigma^2))^{-1} q_1)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_difference_threshold = 0.2\n",
    "rejection_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_difference(data, arm, sig2_rule, sig2_rule_ops, thresh):\n",
    "    n_arms, _ = data.shape\n",
    "    post, x_max, hess, _ = posterior_sigma_sq(data, sig2_rule, sig2_rule_ops)\n",
    "    hess_fn = jax.vmap(lambda h: jnp.diag(h[0]) + jnp.full(shape=(n_arms, n_arms), fill_value=h[1]))\n",
    "    prec = -hess_fn(hess) # (n_sigs, n_arms, n_arms)\n",
    "    order = jnp.arange(0, n_arms)\n",
    "    q1 = jnp.where(order == 0, -1, 0)\n",
    "    q1 = jnp.where(order == arm, 1, q1)\n",
    "    loc = x_max @ q1\n",
    "    scale = jnp.linalg.solve(prec, q1[None,:]) @ q1\n",
    "    normal_term = jax.scipy.stats.norm.cdf(thresh, loc=loc, scale=scale)\n",
    "    post_weighted = sig2_rule.wts * post\n",
    "    out = normal_term @ post_weighted\n",
    "    return out\n",
    "\n",
    "posterior_difference(data, 1, sig2_rule, sig2_rule_ops, posterior_difference_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Probability of Success\n",
    "\n",
    "The next quantity we need to compute is the posterior probability of success (PPS).\n",
    "For convenience of implementation, we will take this to mean the following:\n",
    "let $y, n$ denote the currently observed data\n",
    "and $A_i = \\{ \\text{Phase III rejects using treatment arm i} \\}$.\n",
    "Then, we wish to compute\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(A_i | y, n)\n",
    "\\end{align*}\n",
    "Expanding the quantity,\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(A_i | y, n) &=\n",
    "\\int \\mathbb{P}(A_i | y, n, \\theta_i, \\theta_0) p(\\theta_0, \\theta_i | y, n) \\, d\\theta_i d\\theta_0 \\\\&=\n",
    "\\int \\mathbb{P}(A_i | y, n, \\theta_i, \\theta_0) p(\\theta_0, \\theta_i | y, n) \\, d\\theta_i d\\theta_0\n",
    "\\end{align*}\n",
    "\n",
    "Once we have an estimate for $p(\\theta_0, \\theta_i | y, n)$, \n",
    "we can use 2-D quadrature to numerically integrate the integrand.\n",
    "Similar to computing the probability of best arm,\n",
    "\\begin{align*}\n",
    "p(\\theta_0, \\theta_i | y, n)\n",
    "&=\n",
    "\\int p(\\theta_0, \\theta_i | y, n, \\sigma^2) p(\\sigma^2 | y, n) \\, d\\sigma^2\n",
    "\\end{align*}\n",
    "We will use the Gaussian approximation for $p(\\theta_0, \\theta_i | y, n, \\sigma^2)$\n",
    "and use INLA to estimate $p(\\sigma^2 | y, n)$.\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta | y, n, \\sigma^2)\n",
    "\\approx\n",
    "\\mathcal{N}(\\theta^*, -(H\\log p(\\theta^*, y, \\sigma^2))^{-1})\n",
    "\\\\\n",
    "\\implies\n",
    "p(\\theta_0, \\theta_i | y, n, \\sigma^2)\n",
    "\\approx\n",
    "\\mathcal{N}(\\theta^*_{[0,i]}, -(H\\log p(\\theta^*, y, \\sigma^2))^{-1}_{[0,i], [0,i]})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "n_Ai_sims = 1000\n",
    "p = jnp.full(n_arms, 0.5)\n",
    "n_stage_2 = 100\n",
    "pps_threshold_lower = 0.1\n",
    "pps_threshold_upper = 0.9\n",
    "posterior_difference_threshold = 0.1\n",
    "rejection_threshold = 0.1\n",
    "\n",
    "subset = jnp.array([0, 1])\n",
    "non_futile_idx = np.zeros(n_arms)\n",
    "non_futile_idx[subset] = 1\n",
    "non_futile_idx = jnp.array(non_futile_idx)\n",
    "\n",
    "# create a dense grid of sig2 values\n",
    "n_sig2 = 100\n",
    "sig2_grid = 10**jnp.linspace(-6, 3, n_sig2)\n",
    "dsig2_grid = jnp.diff(sig2_grid)\n",
    "sig2_grid_ops = berry.optimized(sig2_grid, n_arms=data.shape[0]).config(\n",
    "    opt_tol=1e-3\n",
    ")\n",
    "\n",
    "_, key = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_Ai(\n",
    "    data, p, key, best_arm, non_futile_idx, \n",
    "    sig2_rule, sig2_rule_ops,\n",
    "    sig2_grid, sig2_grid_ops, dsig2_grid,\n",
    "):\n",
    "    n_arms, _ = data.shape\n",
    "\n",
    "    # compute p(sig2 | y, n), mode, hessian\n",
    "    p_pinned = dict(sig2=sig2_grid, theta=None)\n",
    "    logpost, x_max, hess, _ = jax.jit(sig2_grid_ops.laplace_logpost)(\n",
    "        np.zeros((len(sig2_grid), n_arms)), p_pinned, data\n",
    "    )\n",
    "    max_logpost = jnp.max(logpost)\n",
    "    max_post = jnp.exp(max_logpost)\n",
    "    post = jnp.exp(logpost - max_logpost) * max_post\n",
    "\n",
    "    # sample sigma^2 | y, n\n",
    "    dFx = post[:-1] * dsig2_grid\n",
    "    Fx = jnp.cumsum(dFx)\n",
    "    Fx /= Fx[-1]\n",
    "    _, key = jax.random.split(key)\n",
    "    unifs = jax.random.uniform(key=key, shape=(n_Ai_sims,))\n",
    "    i_star = jnp.searchsorted(Fx, unifs)\n",
    "\n",
    "    # sample theta | y, n, sigma^2\n",
    "    mean = x_max[i_star+1]\n",
    "    hess_fn = jax.vmap(\n",
    "        lambda h: jnp.diag(h[0]) + jnp.full(shape=(n_arms, n_arms), fill_value=h[1])\n",
    "    )\n",
    "    prec = -hess_fn(hess)\n",
    "    cov = jnp.linalg.inv(prec)[i_star+1]\n",
    "    _, key = jax.random.split(key)\n",
    "    theta = jax.random.multivariate_normal(\n",
    "        key=key, mean=mean, cov=cov,\n",
    "    )\n",
    "    p_samples = jax.scipy.special.expit(theta)\n",
    "\n",
    "    # estimate P(A_i | y, n, theta_0, theta_i)\n",
    "\n",
    "    def simulate_Ai(data, best_arm, diff_thresh, rej_thresh, non_futile_idx, key, p):\n",
    "        # add n_stage_2 number of patients to each\n",
    "        # of the control and selected treatment arms.\n",
    "        n_new = jnp.where(non_futile_idx, n_stage_2, 0)\n",
    "        y_new = dist.Binomial(total_count=n_new, probs=p).sample(key)\n",
    "\n",
    "        # pool outcomes for each arm\n",
    "        data = data + jnp.stack((y_new, n_new), axis=-1)\n",
    "\n",
    "        return posterior_difference(data, best_arm, sig2_rule, sig2_rule_ops, diff_thresh) < rej_thresh\n",
    "\n",
    "    simulate_Ai_vmapped = jax.vmap(\n",
    "        simulate_Ai, in_axes=(None, None, None, None, None, 0, 0)\n",
    "    )\n",
    "    keys = jax.random.split(key, num=p_samples.shape[0])\n",
    "    Ai_indicators = simulate_Ai_vmapped(\n",
    "        data,\n",
    "        best_arm,\n",
    "        posterior_difference_threshold,\n",
    "        rejection_threshold,\n",
    "        non_futile_idx,\n",
    "        keys,\n",
    "        p_samples,\n",
    "    )\n",
    "    out = jnp.mean(Ai_indicators)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jax.jit(lambda data, p, key, best_arm, non_futile_idx:\n",
    "    pr_Ai(data, p, key, best_arm, non_futile_idx, \n",
    "          sig2_rule, sig2_rule_ops, sig2_grid, sig2_grid_ops, dsig2_grid),\n",
    "    static_argnums=(3,))(data, p, key, 1, non_futile_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling based on pdf values and linearly interpolating\n",
    "n_sims = 1000\n",
    "n_unifs = 1000\n",
    "key = jax.random.PRNGKey(2)\n",
    "\n",
    "#x = jnp.linspace(-3, 3, num=n_sims)\n",
    "#px_orig = 0.5*jax.scipy.stats.norm.pdf(x, -1, 0.5) + 0.5*jax.scipy.stats.norm.pdf(x, 1, 0.5)\n",
    "\n",
    "#x = jnp.linspace(0, 10, num=n_sims)\n",
    "#px_orig = jax.scipy.stats.gamma.pdf(x, 10)\n",
    "\n",
    "x = jnp.linspace(0, 1, num=n_sims)\n",
    "px_orig = jax.scipy.stats.beta.pdf(x, 4, 2)\n",
    "\n",
    "px = 2 * px_orig\n",
    "dx = jnp.diff(x)\n",
    "dFx = px[:-1] * dx\n",
    "Fx = jnp.cumsum(dFx)\n",
    "Fx /= Fx[-1]\n",
    "_, key = jax.random.split(key)\n",
    "unifs = jax.random.uniform(key=key, shape=(n_unifs,))\n",
    "i_star = jnp.searchsorted(Fx, unifs)\n",
    "\n",
    "# point mass approx\n",
    "#samples = x[i_star+1]\n",
    "\n",
    "# constant approx\n",
    "#samples = x[i_star+1] - (Fx[i_star] - unifs) / px[i_star]\n",
    "\n",
    "# linear approx\n",
    "a = 0.5 * (px[i_star+1] - px[i_star]) / dx[i_star]\n",
    "b = px[i_star]\n",
    "c = Fx[i_star] - unifs - px[i_star] * dx[i_star] - a * dx[i_star]**2\n",
    "discr = jnp.sqrt(jnp.maximum(b**2 - 4*a*c, 0))\n",
    "quad_solve = jnp.where(jnp.abs(a) < 1e-8, -c/b, (-b + discr) / (2*a))\n",
    "samples = x[i_star] + quad_solve\n",
    "\n",
    "#plt.plot(x[1:], Fx)\n",
    "#plt.plot(x[1:], jax.scipy.stats.norm.cdf(x[1:]))\n",
    "plt.hist(x[i_star+1], density=True, alpha=0.5)\n",
    "plt.hist(samples, density = True, alpha=0.5)\n",
    "plt.plot(x, px_orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.46 ms, sys: 755 Âµs, total: 5.21 ms\n",
      "Wall time: 2.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    \"n_arms\" : 3,\n",
    "    \"n_stage_1\" : 20,\n",
    "    \"n_stage_2\" : 10,\n",
    "    \"n_stage_1_interims\" : 2,\n",
    "    \"n_stage_1_add_per_interim\" : 4,\n",
    "    \"n_stage_2_add_per_interim\" : 4,\n",
    "    \"stage_1_futility_threshold\" : 0.1,\n",
    "    \"stage_1_efficacy_threshold\" : 0.9,\n",
    "    \"stage_2_futility_threshold\" : 0.1,\n",
    "    \"stage_2_efficacy_threshold\" : 0.9,\n",
    "    \"inter_stage_futility_threshold\" : 0.8,\n",
    "    \"posterior_difference_threshold\" : 0.05,\n",
    "    \"rejection_threshold\" : 0.05,\n",
    "    \"key\" : jax.random.PRNGKey(0),\n",
    "    \"n_pr_sims\" : 100,\n",
    "    \"n_sig2_sims\" : 20,\n",
    "    \"batch_size\" : int(2**16),\n",
    "    \"cache_tables\" : False,\n",
    "}\n",
    "lei_obj = lewis.Lewis45(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2**16\n",
    "p = jnp.array([0.5] + [0.9] * (params['n_arms']-1))\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_sims = 1000\n",
    "keys = jax.random.split(key, num=n_sims)\n",
    "n_grid_points = 2\n",
    "grid_points = jnp.array([p] * n_grid_points)\n",
    "n_pr_sims = 100\n",
    "n_sig2_sim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def generate_data(n, p, key, n_sims=-1):\n",
    "    if n_sims == -1:\n",
    "        y = dist.Binomial(n, p).sample(key)\n",
    "        data = jnp.stack((y, n), axis=-1)\n",
    "    else:\n",
    "        y = dist.Binomial(n, p).sample(key, (n_sims,))\n",
    "        n_full = jnp.full_like(y, n)\n",
    "        data = jnp.stack((y, n_full), axis=-1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.38 s, sys: 222 ms, total: 5.6 s\n",
      "Wall time: 4.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lewis.lookup_table.LookupTable at 0x2e956c190>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lei_obj.pd_table = lei_obj.posterior_difference_table__(batch_size=batch_size)\n",
    "lei_obj.pd_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.75 s, sys: 359 ms, total: 10.1 s\n",
      "Wall time: 8.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lewis.lookup_table.LookupTable at 0x2ae03c910>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lei_obj.pr_best_pps_1_table = lei_obj.pr_best_pps_1_table__(\n",
    "    key, \n",
    "    n_pr_sims,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "lei_obj.pr_best_pps_1_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 831 ms, total: 13.8 s\n",
      "Wall time: 10.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lewis.lookup_table.LookupTable at 0x37b2dcb80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lei_obj.pps_2_table = lei_obj.pps_2_table__(\n",
    "    key, \n",
    "    n_pr_sims,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "lei_obj.pps_2_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewis.cartesian_batcher import CartesianBatcher\n",
    "\n",
    "cb = CartesianBatcher(\n",
    "    lower=-1,\n",
    "    upper=1,\n",
    "    n_batches=3,\n",
    "    batch_size=10,\n",
    "    n_arms=params['n_arms'],\n",
    ")\n",
    "\n",
    "indices = cb.batch_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_vmap(i, batcher, unifs, unifs_order):\n",
    "    return jax.vmap(lei_obj.simulate, in_axes=(0, None, None))(\n",
    "        batcher.batch(i), unifs, unifs_order,\n",
    "    )\n",
    "    \n",
    "def each_sim(key, batcher):\n",
    "    unifs = jax.random.uniform(key, shape=lei_obj.unifs_shape())\n",
    "    unifs_order =jnp.arange(0, unifs.shape[0])\n",
    "    grid_1d = jax.scipy.special.expit(batcher.grid_1d)\n",
    "    indices = batcher.batch_indices()\n",
    "    return jax.vmap(\n",
    "        simulate_vmap,\n",
    "        in_axes=(0, None, None, None)\n",
    "    )(indices, batcher, unifs, unifs_order).flatten()\n",
    "    \n",
    "def each_sim_vmap(keys, batcher):\n",
    "    return jax.vmap(each_sim, in_axes=(0, None))(keys, batcher)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def sim_batch(keys, batcher):\n",
    "    return jnp.sum(each_sim_vmap(keys, batcher), axis=0)\n",
    "    \n",
    "def simulate_all(keys, batcher, batch_size):\n",
    "    f = lewis.batch.batch_all(sim_batch, batch_size, in_axes=(0, None))\n",
    "    outs, n_padded = f(keys, batcher)\n",
    "    out = jnp.row_stack(outs)\n",
    "    out = out[:-n_padded] if n_padded > 0 else out\n",
    "    return jnp.sum(out, axis=0) \n",
    "    \n",
    "keys = jax.random.split(key, num=20)\n",
    "each_sim_jit = jax.jit(each_sim, static_argnums=(1,))\n",
    "each_sim_vmap_jit = jax.jit(each_sim_vmap, static_argnums=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#each_sim_jit(key, cb)\n",
    "each_sim_vmap_jit(keys, cb)\n",
    "#sim_batch(keys, cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lewis.lookup_table import LookupTable, HashableArrayWrapper\n",
    "\n",
    "n_configs = jnp.array([\n",
    "    [1, 2, 3],\n",
    "    [10, 2, 1],\n",
    "])\n",
    "tables = tuple(\n",
    "    jnp.arange(0, jnp.prod(ns+1)).reshape((-1, 1))\n",
    "    for ns in n_configs\n",
    ")\n",
    "\n",
    "table = LookupTable(n_configs, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function foo at /var/folders/8w/87ph5tkx60vc7f4l87071knc0000gn/T/ipykernel_38947/815444734.py:3 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:i64[3]\u001b[39m = mul b c\n    from line /Users/jhyang/sandbox/confirmasaurus/research/lei/lewis/lookup_table.py:11 (__hash__)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m@partial\u001b[39m(jax\u001b[39m.\u001b[39mjit, static_argnums\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfoo\u001b[39m(n):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mdict\u001b[39m[n] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m foo(HashableArrayWrapper(n_configs[\u001b[39m0\u001b[39;49m], table\u001b[39m.\u001b[39;49mn_configs_max_mask))\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[1;32m/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb Cell 42\u001b[0m in \u001b[0;36mfoo\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m@partial\u001b[39m(jax\u001b[39m.\u001b[39mjit, static_argnums\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfoo\u001b[39m(n):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jhyang/sandbox/confirmasaurus/research/lei/lei.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mdict\u001b[39;49m[n] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/sandbox/confirmasaurus/research/lei/lewis/lookup_table.py:11\u001b[0m, in \u001b[0;36mHashableArrayWrapper.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(jnp\u001b[39m.\u001b[39;49msum(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask))\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/confirm/lib/python3.10/site-packages/jax/core.py:1174\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1174\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(int64[])>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function foo at /var/folders/8w/87ph5tkx60vc7f4l87071knc0000gn/T/ipykernel_38947/815444734.py:3 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:i64[3]\u001b[39m = mul b c\n    from line /Users/jhyang/sandbox/confirmasaurus/research/lei/lewis/lookup_table.py:11 (__hash__)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def foo(n):\n",
    "    dict[n] = 1\n",
    "\n",
    "foo(HashableArrayWrapper(n_configs[0], table.n_configs_max_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('confirm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8e1ca1b3fede25e3995e2b26ea544fa1b75b9a17984e6284a43c1dc286640dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
