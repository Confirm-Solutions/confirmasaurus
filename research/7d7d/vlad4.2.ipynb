{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case study will explore the examples of Wassmer and Dragalin (2015). This paper uses closed testing methods to ensure Type I Error control, while adapting the trial to focus patient recruitment on the subpopulations where the treatment performs best.\n",
    "\n",
    "There are 3 different case studies in the paper: 4.1, 4.2, and 4.3.\n",
    "4.1 uses binomial outcomes, with a Bonferroni rule, with 2 total subgroups x 2 treatments (T vs C) = 4 parameters. Two analysis times, including the final. This should be improvable.\n",
    "4.2 uses exponential outcomes, does a log-rank test with Dunnett's test for multiple comparisons, with 2 total subgroups x 2 treatments = with 2 total subgroups x 2 treatments (T vs C) = 4 parameters. Four analysis times, including the final. We expect that this one is essentially tight/hard to improve, because it uses Dunnett, which should be tight\n",
    "4.3 uses exponential outcomes, and does log-rank tests with the Bonferroni rule, with 4 subgroups x 2 treatments = 8 parameters. This is too much for proof-by-simulation to cover. We could still give a superior power analysis to what was reported in the paper, which fixes many of these parameters and varies the treatment effects.\n",
    "\n",
    "Of this subset, WE WILL DO: 4.1, and then show that it can also be done under exponential outcomes, in both cases yielding improvements.\n",
    "\n",
    "If we want to beef it up further we could try to compare against 4.2, but I don't think this is strictly necessary to show.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imprint.nb_util import setup_nb\n",
    "\n",
    "# setup_nb is a handy function for setting up some nice plotting defaults.\n",
    "setup_nb()\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imprint as ip\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.special import expit, logit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4.2 has survival outcomes.\n",
    "There are 2 treatment arms and 2 subgroups:\n",
    "40% of patients are Stage II, 60% of patients are Stage III.\n",
    "\n",
    "We will use exponential outcomes to represent overall survival:\n",
    "\n",
    "If all patients start at the same time, the results will implicitly cover the class of proportional hazards models with bounded hazard, by an implicit time-reparametrization, so long as our analysis uses only failure-rank information.\n",
    "\n",
    "(This is not realistic due to run-in, but the degree to which it is violated does not seem to obviously favor treatment or control... clinical trials I believe generally use time-from-treatment-to-event scale. ...hold on a second, this is already a problem in the normal course of things: what do trials currently do for a log-rank test when there is spillover across interims from time-to-treat analysis, where the previous observations effectively change? anyway...)\n",
    "\n",
    "Most of the patients who died were in the Stage III group.\n",
    "\n",
    "Let's assume the sequence of patient arrivals is known (and in the future, that we'll re-visit this analysis with sequential conditioning). This analysis here also effectively conditions on the final number of deaths, anyway.\n",
    "\n",
    "There were 1123 patients assigned to each treatment group/ with a 40/60 split in each of Stage II vs Stage III\n",
    "\n",
    "They give the interims weights driven by observed information rates:\n",
    "information rates:\n",
    "0.53, 0.7, 0.77, 1.0\n",
    "with weights given by \n",
    "sqrt(t_k - t_k-1).\n",
    "\n",
    "The subgroup S is Phase III status.\n",
    "The full group is Phase II + Phase III.\n",
    "\n",
    "They use the log-rank statistics with a normal approximation for the elementary hypotheses.\n",
    "\n",
    "For every hypothesis (including the intersection hypotheses), they will do \"alpha-spending\" which results in critical values:\n",
    "3.090, 2.976, 2.854, 1.968\n",
    "\n",
    "We will tune the final of these with calibration, to measure our loss.\n",
    "\n",
    "To get z-stats, we consider increments of the log-rank test: if d_k is the number of events in the log-rank test,\n",
    "\n",
    "Z = [sqrt(d_k) LR_k - sqrt(d_{k-1}) LR_{k-1} ] / sqrt(d_k - d_{k-1})\n",
    "\n",
    "is used as a z-test\n",
    "\n",
    "For the intersection hypotheses, we use Dunnet's test on the two Z-stats:\n",
    "\n",
    "p = 1 - F_{sigma,df} (max(Z_1, Z_2))\n",
    "\n",
    "Where sigma is a correlation matrix with diagonals of 1, and the off-diagonals\n",
    "\n",
    "sqrt(n0_S + n1_S)/sqrt(n0_F + n1_F)\n",
    "\n",
    "So basically the covariance matrix is the ratio of total sample sizes in the small/larger-containing group, where \"sample sizes\" counts only EVENT FAILURES.\n",
    "\n",
    "We'll just use a normal approximation ourselves, with the code:\n",
    "\n",
    "import scipy.stats\n",
    "c = 0.5\n",
    "1 - scipy.stats.multivariate_normal.cdf([c, c], mean=[0, 0], cov=[[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FOLFOX4 group is our treatment\n",
    "# LV5FU2 is our control\n",
    "# We also have our subgroups, stage 2 or 3 cancer.\n",
    "# We list their estimated hazard values here (used in the text)\n",
    "\n",
    "# Now, assuming that the trial will be cut at 528 failures, \n",
    "# WLOG we can set hcontrol_3 = 1\n",
    "hcontrol_3 = 1\n",
    "htreat_3 = 0.9\n",
    "hcontrol_2 = 0.5\n",
    "htreat_2 = 0.5\n",
    "# I'm guessing these numbers will give us about similar outcomes to what was observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to make a list of the relevant \n",
    "# intersection hypotheses, \n",
    "#which are indentified by which subgroups are part of them\n",
    "# In this example, there are 2 elementary hypotheses\n",
    "hypo3_live = True\n",
    "hypofull_live = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_list.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll repeat this for the second half, which is the same size as the first half\n",
    "npatientstotal = 1128*2\n",
    "npatients_2_perarm = int(1128 * 0.4)\n",
    "npatients_3_perarm = int(1128 * 0.6) + 1\n",
    "treat2outcomes = scipy.stats.expon.rvs(size = npatients_2_perarm) / htreat_2\n",
    "ctrl2outcomes = scipy.stats.expon.rvs(size = npatients_2_perarm) / hcontrol_2\n",
    "ctrl3outcomes = scipy.stats.expon.rvs(size = npatients_3_perarm)/ hcontrol_3\n",
    "treat3outcomes = scipy.stats.expon.rvs(size = npatients_3_perarm) / htreat_3\n",
    "\n",
    "#\n",
    "information_weights = [0.53, 0.7, 0.77, 1.0]\n",
    "# We now determine the relevant stopping times\n",
    "stopping_points = [int(528 * x) for x in information_weights]\n",
    "\n",
    "# crying. I'm sorry. I'm so sorry.\n",
    "outcomes_list = np.append(np.append(np.append(ctrl2outcomes, ctrl3outcomes), treat3outcomes),treat2outcomes)\n",
    "sorted = np.sort(outcomes_list)\n",
    "stoptimes = [sorted[stopping_points[i]] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[597, 789, 868, 1128]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The adult way to do this would be to edit the following logrank function, \n",
    "#to switch out censoring time for a sequence of censoring times, and each time return the relevant z-statistics.\n",
    "# Because the following is already vectorized and so close to what we want, \n",
    "# I'm tempted to just leave it this way and code on ahead without making a runnable single-unit version\n",
    "\n",
    "@jax.jit\n",
    "def logrank_test(all_rvs, group, censoring_time):\n",
    "    n0 = jnp.array([jnp.sum(~group), jnp.sum(group)])\n",
    "    ordering = jnp.argsort(all_rvs)\n",
    "    ordered = all_rvs[ordering]\n",
    "    ordered_group = group[ordering]\n",
    "    include = ordered <= censoring_time\n",
    "    event_now = jnp.stack((~ordered_group, ordered_group), axis=0) * include\n",
    "    events_so_far = jnp.concatenate(\n",
    "        (jnp.zeros((2, 1)), event_now.cumsum(axis=1)[:, :-1]), axis=1\n",
    "    )\n",
    "    Nij = n0[:, None] - events_so_far\n",
    "    Oij = event_now\n",
    "    Nj = Nij.sum(axis=0)\n",
    "    Oj = Oij.sum(axis=0)\n",
    "    Eij = Nij * (Oj / Nj)\n",
    "    Vij = Eij * ((Nj - Oj) / Nj) * ((Nj - Nij) / (Nj - 1))\n",
    "    denom = jnp.sum(jnp.where(~jnp.isnan(Vij[0]), Vij[0], 0), axis=0)\n",
    "    return jnp.sum(Oij[0] - Eij[0], axis=0) / jnp.sqrt(denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, these functions shouldn't run, but they're already in multidimensional form, and need to just be modified to output\n",
    "# multiple logrank-statistics as of each different censoring point\n",
    "#z_3_elementary = logrank_test([treat3outcomes, ctrl3outcomes], group = blah, censoring_times = stopping_points)\n",
    "#z_full_elementary = logrank_test([treat3outcomes + treat2outcomes, ctrl3outcomes+ ctrl2outcomes], group = blah, censoring_times = stopping_points)\n",
    "\n",
    "# Putting in placeholder stats\n",
    "Logrank_stats_3 = [1.2, 1.67, 1.349, 1.73]\n",
    "Logrank_stats_full = [0.3746, 0.87, 0.55, 1.11]\n",
    "\n",
    "# Now we compute independent z_statistics\n",
    "num_3 = np.diff(np.append(0, np.sqrt(information_weights) * Logrank_stats_3))\n",
    "denom_3 = np.sqrt(np.diff(np.append(0, information_weights)))\n",
    "z_3_elementary = num_3/denom_3\n",
    "\n",
    "num_full = np.diff(np.append(0, np.sqrt(information_weights) * Logrank_stats_full))\n",
    "denom_full = np.sqrt(np.diff(np.append(0, information_weights)))\n",
    "z_full_elementary = num_full/denom_full\n",
    "\n",
    "# Now we compute the p-values for each period\n",
    "p_3_elementary = 1-scipy.stats.norm.cdf(z_3_elementary)\n",
    "p_full_elementary = 1-scipy.stats.norm.cdf(z_full_elementary)\n",
    "\n",
    "p_intersection = np.full_like(p_3_elementary, 2)\n",
    "\n",
    "# doing this for the covariance:\n",
    "for i in range(4):\n",
    "    if(i == 0):\n",
    "        numfailures_3 = sum(treat3outcomes <= stoptimes[i]) + sum(ctrl3outcomes <= stoptimes[i])\n",
    "        numfailures_total = stopping_points[i]\n",
    "    if(i > 0):\n",
    "        numfailures_3 = sum(treat3outcomes <= stoptimes[i]) - sum(treat3outcomes <= stoptimes[i-1]) + sum(ctrl3outcomes <= stoptimes[i]) - sum(ctrl3outcomes <= stoptimes[i-1])\n",
    "        numfailures_total = stopping_points[i] - stopping_points[i-1]\n",
    "    offdiag = numfailures_3 / numfailures_total\n",
    "    c = np.max(np.append(z_3_elementary[i], z_full_elementary[i]))\n",
    "    p_intersection[i] = 1 - scipy.stats.multivariate_normal.cdf([c, c], mean=[0, 0], cov=[[1, offdiag], [offdiag, 1]])\n",
    "# In fact there is no selection! So we simply use Dunnett on the elementary hypotheses here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2       ,  1.2699385 , -0.80687685,  1.13902522])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we perform weighted combination, according to the information weights. No subsets can be dropped, so this is a simple weighted sum.\n",
    "info_decomp = np.append(information_weights[0],np.diff(information_weights))\n",
    "z_weights = np.sqrt(info_decomp)\n",
    "p_3_combined = 1 - scipy.stats.norm.cdf (np.sum(scipy.stats.norm.ppf(1 - p_3_elementary) * z_weights))\n",
    "p_full_combined = 1 - scipy.stats.norm.cdf (np.sum(scipy.stats.norm.ppf(1 - p_full_elementary) * z_weights))\n",
    "p_intersection_combined = 1 - scipy.stats.norm.cdf (np.sum(scipy.stats.norm.ppf(1 - p_intersection) * z_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3746    ,  1.10397683, -0.92703793,  1.30817139])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute final rejection from closed testing:\n",
    "alpha = 0.05\n",
    "reject_3 = (p_intersection_combined < alpha) & (p_3_combined < alpha)\n",
    "reject_full = (p_intersection_combined < alpha) & (p_full_combined < alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRplus_pooledaverage = outcomesHRplustreat.mean/2 + outcomesHRpluscontrol.mean/2\n",
    "#denominatorHRplus = np.sqrt(HRplus_pooledaverage*(1-HRplus_pooledaverage)*(1/npatientsperarm + 1/npatientsperarm))\n",
    "#zHRplus_stage1 = (outcomesHRplustreat.mean - outcomesHRpluscontrol.mean)/denominatorHRplus\n",
    "\n",
    "# Note: this could be expressed more simply as a single function if we could nicely combine treatment outcome columns. but whatever...\n",
    "TNBC_pooledaverage = outcomesTNBCtreat.mean/2 + outcomesTNBCcontrol.mean/2\n",
    "denominatorTNBC = np.sqrt(TNBC_pooledaverage*(1-TNBC_pooledaverage)*(1/npatientsperarm_firststage + 1/npatientsperarm_firststage))\n",
    "zTNBC_stage1 = (outcomesTNBCtreat.mean - outcomesTNBCcontrol.mean)/denominatorTNBC\n",
    "\n",
    "totally_pooledaverage = outcomesTNBCtreat.mean/4 + outcomesTNBCcontrol.mean/4 + outcomesHRplustreat.mean/4 + outcomesHRpluscontrol.mean/4\n",
    "denominatortotallypooled = np.sqrt(totally_pooledaverage*(1-totally_pooledaverage)*(1/(2*npatientsperarm_firststage) + 1/(2*npatientsperarm_firststage)))\n",
    "zfull_stage1 = (outcomesTNBCtreat.mean/2 + outcomesHRplustreat.mean/2 - outcomesTNBCcontrol.mean/2 - outcomesHRpluscontrol.mean/2)/denominatortotallypooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done! Just ignore the below scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([          nan, 5.2642647e-02,           nan, 5.2622620e-02,           nan,\n",
       "                       nan,           nan,           nan, 5.2641593e-02, 2.2225672e-01, ...,\n",
       "                       nan, 5.2633159e-02,           nan, 5.2621566e-02,           nan,\n",
       "                       nan, 2.8201441e-09,           nan,           nan,           nan],            dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogRankVlad:\n",
    "    def __init__(self, seed, max_K, *, n, censoring_time):\n",
    "        self.max_K = max_K\n",
    "        self.censoring_time = censoring_time\n",
    "        self.n = n\n",
    "        self.family = \"exponential\"\n",
    "        self.family_params = {\"n\": n}\n",
    "\n",
    "        self.key = jax.random.PRNGKey(seed)\n",
    "        self.samples_1 = jax.random.exponential(self.key, shape=(max_K, n, 4))\n",
    "        self.group = jnp.concatenate(\n",
    "            [jnp.zeros((max_K, n)), jnp.ones((max_K, n))], axis=1\n",
    "        ).astype(bool)\n",
    "        self.vmap_logrank_test = jax.vmap(\n",
    "            jax.vmap(logrank_test, in_axes=(0, 0, None)), in_axes=(0, None, None)\n",
    "        )\n",
    "\n",
    "    def sim_batch(self, begin_sim, end_sim, theta, null_truth, detailed=False):\n",
    "        control_hazard = -theta[:, 0]\n",
    "        treatment_hazard = -theta[:, 1]\n",
    "        hazard_ratio = treatment_hazard / control_hazard\n",
    "        control_rvs = jnp.tile(self.samples[None, :, :, 0], (hazard_ratio.shape[0], 1, 1))\n",
    "        treatment_rvs = self.samples[None, :, :, 1] / hazard_ratio[:, None, None]\n",
    "        all_rvs = jnp.concatenate([control_rvs, treatment_rvs], axis=2)\n",
    "        test_stat = -self.vmap_logrank_test(\n",
    "            all_rvs, self.group, self.censoring_time\n",
    "        )\n",
    "        return test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ip.cartesian_grid([-1, -1], [-1, -1], n=[1, 1], null_hypos=[ip.hypo(\"theta0 > theta1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogRank(0, 200000, n=100, censoring_time=10000000)\n",
    "stats = lr.sim_batch(0, lr.max_K, g.get_theta(), g.get_null_truth())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confirm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244a70f8ba587ac945a75283a2462feda5931af2453571227e17a5a68b0d3888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
