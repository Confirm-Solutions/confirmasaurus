{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case study will explore the examples of Wassmer and Dragalin (2015). This paper uses closed testing methods to ensure Type I Error control, while adapting the trial to focus patient recruitment on the subpopulations where the treatment performs best.\n",
    "\n",
    "There are 3 different case studies in the paper: 4.1, 4.2, and 4.3.\n",
    "4.1 uses binomial outcomes, with a Bonferroni rule, with 2 total subgroups x 2 treatments (T vs C) = 4 parameters. Two analysis times, including the final. This should be improvable.\n",
    "4.2 uses exponential outcomes, does a log-rank test with Dunnett's test for multiple comparisons, with 2 total subgroups x 2 treatments = with 2 total subgroups x 2 treatments (T vs C) = 4 parameters. Four analysis times, including the final. We expect that this one is essentially tight/hard to improve, because it uses Dunnett, which should be tight\n",
    "4.3 uses exponential outcomes, and does log-rank tests with the Bonferroni rule, with 4 subgroups x 2 treatments = 8 parameters. This is too much for proof-by-simulation to cover. We could still give a superior power analysis to what was reported in the paper, which fixes many of these parameters and varies the treatment effects.\n",
    "\n",
    "Of this subset, WE WILL DO: 4.1, and then show that it can also be done under exponential outcomes, in both cases yielding improvements.\n",
    "\n",
    "If we want to beef it up further we could try to compare against 4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imprint.nb_util import setup_nb\n",
    "\n",
    "# setup_nb is a handy function for setting up some nice plotting defaults.\n",
    "setup_nb()\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imprint as ip\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.special import expit, logit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will replicate example 4.1\n",
    "The population breakdown is 54% HRplus, 46% no-HRplus. [Let's assumed blocked sample sizes, for simplicity; and note that this anyway describes how the problem would be approached with post-conditioning]\n",
    "\n",
    "\n",
    "The null hypotheses here are:\n",
    "(1) HRplus > 0.34\n",
    "(2) 0.54 p_HRplus + 0.46 p_HER+ > 0.54 * 0.34 + 0.46 * 0.23\n",
    "\n",
    "A decision will be made after 150 patients. There is no early-stopping, but the trial can either select the HRplus group, or continue to the final analysis. This selection occurs if the estimated effect size of HRplus = p-hat_HRplus - 0.34 > epsilon + p-hat_average - (0.34*.54 + 0.23*0.46)\n",
    "where epsilon = 10%\n",
    "\n",
    "If the HER+ group is dropped, then its p-value automatically jumps to 1 and stays there(cannot-reject) but it is still present for the closed test.\n",
    "\n",
    "The p-values for the closed test are determined by inference on the binomial using the z-approximation with pooling treatment and control:\n",
    "\n",
    "Z = pi-hat_1 - pi-hat_0 /sqrt(pi-hat-combined * (1 - pi-hat-combined)*(1/n_1 + 1/n_2))\n",
    "\n",
    "We will do closed testing on the p-values from these two z-statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some sub-methods, to perform the methods given one binomial setting\n",
    "pcontrol_TNBC = 0.34\n",
    "ptreat_TNBC = 0.38\n",
    "pcontrol_HRplus = 0.23\n",
    "ptreat_HRplus = 0.27\n",
    "\n",
    "# generate hypotheses: for this, we have to do stuff similar to the closedtesting.pynb\n",
    "# There will be 2 hypotheses, one of which will have to be identified with a nonlinear equation in theta-space\n",
    "# Let's leave that to Ben, and just focus on executing the design\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have to make a list of the relevant \n",
    "# intersection hypotheses, \n",
    "#which are indentified by which subgroups are part of them\n",
    "# In this example, there are 2 elementary hypotheses\n",
    "hypoTNBC_live = True\n",
    "hypofull_live = True\n",
    "# There is also the intersection hypothesis:\n",
    "# hypo_both_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll repeat this for the second half, which is the same size as the first half\n",
    "npatientstotal = 600\n",
    "npatientsperarm_firststage = 75 #for the first interim analysis\n",
    "unifsTNBCcontrol = np.random.uniform(size = npatientsperarm_firststage)\n",
    "unifsTNBCtreat = np.random.uniform(size = npatientsperarm_firststage)\n",
    "unifsHRpluscontrol = np.random.uniform(size = npatientsperarm_firststage)\n",
    "unifsHRplustreat = np.random.uniform(size = npatientsperarm_firststage)\n",
    "outcomesHRpluscontrol = unifsHRpluscontrol < pcontrol_HRplus\n",
    "outcomesHRplustreat = unifsHRplustreat < ptreat_HRplus\n",
    "outcomesTNBCcontrol = unifsTNBCcontrol < pcontrol_TNBC\n",
    "outcomesTNBCtreat = unifsTNBCtreat < ptreat_TNBC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRplus_pooledaverage = outcomesHRplustreat.mean/2 + outcomesHRpluscontrol.mean/2\n",
    "#denominatorHRplus = np.sqrt(HRplus_pooledaverage*(1-HRplus_pooledaverage)*(1/npatientsperarm + 1/npatientsperarm))\n",
    "#zHRplus_stage1 = (outcomesHRplustreat.mean - outcomesHRpluscontrol.mean)/denominatorHRplus\n",
    "\n",
    "# Note: this could be expressed more simply as a single function if we could nicely combine treatment outcome columns. but whatever...\n",
    "TNBC_pooledaverage = outcomesTNBCtreat.mean/2 + outcomesTNBCcontrol.mean/2\n",
    "denominatorTNBC = np.sqrt(TNBC_pooledaverage*(1-TNBC_pooledaverage)*(1/npatientsperarm_firststage + 1/npatientsperarm_firststage))\n",
    "zTNBC_stage1 = (outcomesTNBCtreat.mean - outcomesTNBCcontrol.mean)/denominatorTNBC\n",
    "\n",
    "totally_pooledaverage = outcomesTNBCtreat.mean/4 + outcomesTNBCcontrol.mean/4 + outcomesHRplustreat.mean/4 + outcomesHRpluscontrol.mean/4\n",
    "denominatortotallypooled = np.sqrt(totally_pooledaverage*(1-totally_pooledaverage)*(1/(2*npatientsperarm_firststage) + 1/(2*npatientsperarm_firststage)))\n",
    "zfull_stage1 = (outcomesTNBCtreat.mean/2 + outcomesHRplustreat.mean/2 - outcomesTNBCcontrol.mean/2 - outcomesHRpluscontrol.mean/2)/denominatortotallypooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arm-dropping logic: drop all elementary hypotheses with larger than 0.1 difference in effect size\n",
    "effectsize_difference = (outcomesTNBCtreat.mean - outcomesTNBCcontrol.mean) - (outcomesHRplustreat.mean - outcomesHRpluscontrol.mean)\n",
    "if effectsize_difference > 0.1:\n",
    "    hypoHRplus_live = False\n",
    "if effectsize_difference < -0.1\n",
    "    hypofull_live = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compute second-stage z-statistics\n",
    "if !hypofull_live: # In this case we drop the HRplus arm\n",
    "    npatientsperarm_secondstage = (npatientstotal - npatientsperarm_firststage*4) / 2 # should be 150: we now have only 2 arms\n",
    "    unifsTNBCcontrol = np.random.uniform(size = npatientsperarm_secondstage)\n",
    "    unifsTNBCtreat = np.random.uniform(size = npatientsperarm_secondstage)\n",
    "    unifsHRpluscontrol = np.random.uniform(size = npatientsperarm_secondstage)\n",
    "    outcomesTNBCcontrol = unifsTNBCcontrol < pcontrol_TNBC\n",
    "    outcomesTNBCtreat = unifsTNBCtreat < ptreat_TNBC\n",
    "    TNBC_pooledaverage = outcomesTNBCtreat.mean/2 + outcomesTNBCcontrol.mean/2\n",
    "    denominatorTNBC = np.sqrt(TNBC_pooledaverage*(1-TNBC_pooledaverage)*(1/npatientsperarm_secondstage + 1/npatientsperarm_secondstage))\n",
    "    zTNBC_stage2 = (outcomesTNBCtreat.mean - outcomesTNBCcontrol.mean)/denominatorTNBC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else # We keep both arms\n",
    "    npatientsperarm = (npatientstotal - npatientsperarm_firststage*4) / 4 # for the second interim analysis, we keep all 4 arms, and the 300 patients are split evenly\n",
    "    unifsTNBCcontrol = np.random.uniform(size = npatientsperarm)\n",
    "    unifsTNBCtreat = np.random.uniform(size = npatientsperarm)\n",
    "    unifsHRpluscontrol = np.random.uniform(size = npatientsperarm)\n",
    "    unifsHRplustreat = np.random.uniform(size = npatientsperarm)\n",
    "    outcomesHRpluscontrol = unifsHRpluscontrol < pcontrol_HRplus\n",
    "    outcomesHRplustreat = unifsHRplustreat < ptreat_HRplus\n",
    "    outcomesTNBCcontrol = unifsTNBCcontrol < pcontrol_TNBC\n",
    "    outcomesTNBCtreat = unifsTNBCtreat < ptreat_TNBC\n",
    "    TNBC_pooledaverage = outcomesTNBCtreat.mean/2 + outcomesTNBCcontrol.mean/2\n",
    "    denominatorTNBC = np.sqrt(TNBC_pooledaverage*(1-TNBC_pooledaverage)*(1/npatientsperarm + 1/npatientsperarm))\n",
    "    zTNBC_stage2 = (outcomesTNBCtreat.mean - outcomesTNBCcontrol.mean)/denominatorTNBC\n",
    "    totally_pooledaverage = outcomesTNBCtreat.mean/4 + outcomesTNBCcontrol.mean/4 + outcomesHRplustreat.mean/4 + outcomesHRpluscontrol.mean/4\n",
    "    denominatortotallypooled = np.sqrt(totally_pooledaverage*(1-totally_pooledaverage)*(1/(150) + 1/(150)))\n",
    "    zfull_stage2 = (outcomesTNBCtreat.mean/2 + outcomesHRplustreat.mean/2 - outcomesTNBCcontrol.mean/2 - outcomesHRpluscontrol.mean/2)/denominatortotallypooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now combine test statistics\n",
    "if !hypofull_live\n",
    "    zfull_stage2 = np.Inf\n",
    "\n",
    "# Now we go through the 3 intersection tests:\n",
    "hypTNBC_zstat = zTNBC_stage1/np.sqrt(2) + zTNBC_stage2/np.sqrt(2)\n",
    "hypfull_zstat = zfull_stage1/np.sqrt(2) + zfull_stage2/np.sqrt(2)\n",
    "\n",
    "# Now doing the combination rule for the intersection test\n",
    "hypintersection_pfirst = 2*np.min(zTNBC_stage1,zfull_stage1)\n",
    "hypintersection_zsecond = hypofull_live * hypoTNBC_live * norm.ppf(2*np.min(zTNBC_stage2,zfull_stage2)) #and do it here too!\n",
    "hypintersection_zsecond += !hypofull_live * hypoTNBC_live * zTNBC_stage2\n",
    "hypintersection_zsecond += hypofull_live * !hypoTNBC_live * zfull_stage2\n",
    "hypintersection_zcombined = norm.ppf(hypintersection_pfirst)/np.sqrt(2) + hypintersection_zsecond/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we resolve which elementary statistics actually reject the null hypothesis\n",
    "rejectintersection = hypintersection_zstat > 1.96\n",
    "rejectTNBC_elementary = hypTNBC_zstat > 1.96\n",
    "rejectfull_elementary = hypfull_zstat > 1.96\n",
    "\n",
    "rejectTNBC_final = rejectintersection & rejectintersection #we use this for actual hypothesis rejections!\n",
    "rejectfull_final = rejectintersection & rejectintersection #we use this for actual hypothesis rejections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP HERE! THe below is just random scratch code, which might be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ip.cartesian_grid(\n",
    "        [-1, 1], [-1, 1], n=[20, 20], prune=True, null_hypos=[ip.hypo(\"theta0 < \"+str(logit(0.3))), ip.hypo(\"theta1 <\" +str(logit(0.3)))]\n",
    "    )\n",
    "#x=BinomialClosed(0, 2, n = [100,100])\n",
    "#outs = x.sim_batch(0, 2, theta = grid.get_theta(), null_truth=grid.df[[\"null_truth0\", \"null_truth1\", \"null_truth2\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinomialClosed:\n",
    "    def __init__(self, seed, max_K, *, n):\n",
    "        self.family = \"binomial\"\n",
    "        self.family_params = {\"n\": n[0]}\n",
    "        self.n=n\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        splitkeys = jax.random.split(key, num=2)\n",
    "        self.samples_arm1 = jax.random.uniform(\n",
    "            splitkeys[1], shape=(max_K, n[1]), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.samples_arm0 = jax.random.uniform(\n",
    "            splitkeys[0], shape=(max_K, n[0]), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def sim_batch(self, begin_sim, end_sim, theta, null_truth, detailed=False):\n",
    "        p = expit(theta)\n",
    "        # successes is a 2-dimensional array of shape: (n_tiles, n_sims = K)\n",
    "        successes1 = np.sum(self.samples_arm1[None, begin_sim:end_sim, :] < p[:, None, None, 1],axis =2)\n",
    "        successes0 = np.sum(self.samples_arm0[None, begin_sim:end_sim, :] < p[:, None, None, 0],axis =2)\n",
    "        phat1 = successes1 / self.n[1]\n",
    "        phat0 = successes0 / self.n[0]\n",
    "        #pooledphat = (phat1*self.n[1] + phat0*self.n[0]) / (self.n[1] + self.n[0])\n",
    "        zstat0 = (phat0 - 0.5) / np.sqrt(phat0 * (1 - phat0) / self.n[0])\n",
    "        zstat1 = (phat1 - 0.7)/ np.sqrt(phat1 * (1 - phat1) / self.n[1])\n",
    "        zstatcombo = (0.4 * (phat0 -0.5)+ 0.6* (phat1- 0.7)) / np.sqrt( 0.4**2 * phat0*(1-phat0)/ self.n[0] + 0.6**2 * phat1*(1-phat1)/ self.n[1])\n",
    "        pvalues = 1-scipy.stats.norm.cdf([zstat0,zstat1,zstatcombo])\n",
    "        pvalues = np.nan_to_num(pvalues) + ([phat0,phat1,phat1 * phat0] == np.full_like(pvalues,0))\n",
    "        # The following needs to be vectorized\n",
    "        possible_critical_values = np.sort(np.concatenate((pvalues,2*pvalues,3*pvalues,np.full_like(pvalues[1,:,:],100)[None,:,:])), axis = 0)\n",
    "        #next we need to apply closed testing to the 3 pvalues for EVERY choice in the list of possible critical values\n",
    "        rejections_list = closed_test_full(pvalues, possible_critical_values)\n",
    "        nullstatus = np.asarray(np.transpose(null_truth))\n",
    "        falserejs = np.logical_and(rejections_list,nullstatus[None,:,:,None])\n",
    "        fwer = np.any(falserejs, axis = 1)\n",
    "        #some akwardness to cause the following minimum to give us the right answer\n",
    "        temp = 1000*(1 - fwer) + fwer*possible_critical_values\n",
    "        out = np.min(temp, axis=0)\n",
    "        #out.shape : n_tiles, n_sims\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:imprint.grid:[worker_id=None] \n",
      "_gen_short_uuids(n=400, worker_id=1, t=1674863641, n_bits=18, worker_bits=18) = [4415288571414708224 4415288571414708225 4415288571414708226, ...]:\n"
     ]
    }
   ],
   "source": [
    "#Make sure to grid -lambda space!\n",
    "grid = ip.cartesian_grid(\n",
    "        [-2.0, -1.0], [-2.0, -1.0], n=[20, 20], prune=True, null_hypos=[ip.hypo(\"theta0 < theta1\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.vmap\n",
    "@jax.jit\n",
    "def logrank_test(all_rvs, group, censoring_time):\n",
    "    n0 = jnp.array([jnp.sum(~group), jnp.sum(group)])\n",
    "    ordering = jnp.argsort(all_rvs)\n",
    "    ordered = all_rvs[ordering]\n",
    "    ordered_group = group[ordering]\n",
    "    include = ordered <= censoring_time\n",
    "    event_now = jnp.stack((~ordered_group, ordered_group), axis=0) * include\n",
    "    events_so_far = jnp.concatenate(\n",
    "        (jnp.zeros((2, 1)), event_now.cumsum(axis=1)[:, :-1]), axis=1\n",
    "    )\n",
    "    Nij = n0[:, None] - events_so_far\n",
    "    Oij = event_now\n",
    "    Nj = Nij.sum(axis=0)\n",
    "    Oj = Oij.sum(axis=0)\n",
    "    Eij = Nij * (Oj / Nj)\n",
    "    Vij = Eij * ((Nj - Oj) / Nj) * ((Nj - Nij) / (Nj - 1))\n",
    "    denom = jnp.sum(jnp.where(~jnp.isnan(Vij[0]), Vij[0], 0), axis=0)\n",
    "    return jnp.sum(Oij[0] - Eij[0], axis=0) / jnp.sqrt(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing pseudocode for Wassmer and Dragalin (2015)\n",
    "# Let's do example 4.3 but applied to the subgroups of 4.1\n",
    "# Calculate worst-possible lengths: max sample size is 300; 54% HRplus / 46% split\n",
    "# Control rates are assumed to be: 0.34 HRplus, 0.23 otherwise. \n",
    "# We can enforce this for now, will grid it out later\n",
    "\n",
    "#(1) Generate max-samples in 4 columns: [HRplus T, HRplus C, HR+ T, HT+ C]\n",
    "#(2) Check effect size after first 150 patients; if difference > 0.1, drop the loser\n",
    "#(3) Do a z-combination test of the log-rank stats from the two time-points\n",
    "#(4) do closed testing if both groups are live. alpha = 0.025\n",
    "#(5) the intersection test\n",
    "\n",
    "#start the above with validation on treatment effects grid\n",
    "#then do it validating over \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 72.,  72.,  90.,  90., 108., 108.,  30.,  30.])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalpatients = 600\n",
    "# Groups list:\n",
    "# 1: !HRplus ^ PH-; 2: HRplus ^ PH+; 3: !HRplus ^ PH+; 4: HRplus ^ PH-\n",
    "# The 4 groups to analyze are:\n",
    "# F = All groups, 1+2+3+4\n",
    "# S1: HRplus = 2 + 4\n",
    "# S2: PHS = 2 + 3\n",
    "# S3: HRplus + PHS = 2\n",
    "popfractions = [0.24,0.3,0.36,0.10]\n",
    "samplesizes = totalpatients*np.array(popfractions)\n",
    "#These will have to be halved for treatment and control later!\n",
    "np.repeat(samplesizes,2)/2 #input sample sizes!\n",
    "#subgroup 0: all groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Steps of the design:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item Determine the nullspace:\n",
    " In this case, we're going to analyze \n",
    " closed testing's Type I Error separately for each intersection hypothesis. For conservatism, we'll assume the alternative guy trips the test, and is retained in the design to increase variability, but also does not eliminate any of the other arms (?)\n",
    " ...oh no I think this doesn't work! Backing up!!!\n",
    "\\item\n",
    " Simulate exponentials\n",
    " Determine the interim analysis time: first 150 events\n",
    " Perform the log-rank test on that subset of events\n",
    " Use the epsilon-selection to select arms:\n",
    " 0.667 > (control events / treat events)_best - (control events / treat events)\n",
    " Eliminate hypotheses completely for dropped arms\n",
    " Determine the final analysis time: first 151-300 events\n",
    " Determine the final analysis time: 300 events\n",
    "\n",
    " Normal combination test (sqrt(2)) factor\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxN = [int(HRplus_selected_final_samps[0]), HRplus_selected_final_samps[1], HER_selected_final_samps[2], HER_selected_final_samps[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "rvs = scipy.stats.expon.rvs(size=(maxN[0], 6))\n",
    "hazard_ratio = 1\n",
    "control_HRplus = rvs[:,0]\n",
    "control_HER = rvs[:,1]\n",
    "control_group3 = rvs[:,2]\n",
    "treat_HRplus = rvs[:,1]/ hazard_ratio\n",
    "treat_HER = rvs[:,3]/hazard_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logrank_2group(control,treatment):\n",
    "    outcomes = np.concatenate([control, treatment])\n",
    "    group = np.concatenate([np.zeros(control.shape[0]), np.ones(treatment.shape[0])]).astype(bool)\n",
    "    ours = logrank_test(outcomes, group, censoring_time=10000)\n",
    "    return ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = logrank_2group(control_HRplus, treat_HRplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.5558844, dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([          nan, 5.2642647e-02,           nan, 5.2622620e-02,           nan,\n",
       "                       nan,           nan,           nan, 5.2641593e-02, 2.2225672e-01, ...,\n",
       "                       nan, 5.2633159e-02,           nan, 5.2621566e-02,           nan,\n",
       "                       nan, 2.8201441e-09,           nan,           nan,           nan],            dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogRankVlad:\n",
    "    def __init__(self, seed, max_K, *, n, censoring_time):\n",
    "        self.max_K = max_K\n",
    "        self.censoring_time = censoring_time\n",
    "        self.n = n\n",
    "        self.family = \"exponential\"\n",
    "        self.family_params = {\"n\": n}\n",
    "\n",
    "        self.key = jax.random.PRNGKey(seed)\n",
    "        self.samples_1 = jax.random.exponential(self.key, shape=(max_K, n, 4))\n",
    "        self.group = jnp.concatenate(\n",
    "            [jnp.zeros((max_K, n)), jnp.ones((max_K, n))], axis=1\n",
    "        ).astype(bool)\n",
    "        self.vmap_logrank_test = jax.vmap(\n",
    "            jax.vmap(logrank_test, in_axes=(0, 0, None)), in_axes=(0, None, None)\n",
    "        )\n",
    "\n",
    "    def sim_batch(self, begin_sim, end_sim, theta, null_truth, detailed=False):\n",
    "        control_hazard = -theta[:, 0]\n",
    "        treatment_hazard = -theta[:, 1]\n",
    "        hazard_ratio = treatment_hazard / control_hazard\n",
    "        control_rvs = jnp.tile(self.samples[None, :, :, 0], (hazard_ratio.shape[0], 1, 1))\n",
    "        treatment_rvs = self.samples[None, :, :, 1] / hazard_ratio[:, None, None]\n",
    "        all_rvs = jnp.concatenate([control_rvs, treatment_rvs], axis=2)\n",
    "        test_stat = -self.vmap_logrank_test(\n",
    "            all_rvs, self.group, self.censoring_time\n",
    "        )\n",
    "        return test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ip.cartesian_grid([-1, -1], [-1, -1], n=[1, 1], null_hypos=[ip.hypo(\"theta0 > theta1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogRank(0, 200000, n=100, censoring_time=10000000)\n",
    "stats = lr.sim_batch(0, lr.max_K, g.get_theta(), g.get_null_truth())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confirm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:54) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244a70f8ba587ac945a75283a2462feda5931af2453571227e17a5a68b0d3888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
