{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import confirm.outlaw.nb_util as nb_util\n",
    "\n",
    "nb_util.setup_nb(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import scipy.spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from confirm.mini_imprint import grid\n",
    "from confirm.lewislib import grid as lewgrid\n",
    "from confirm.lewislib import lewis, batch\n",
    "from confirm.mini_imprint import binomial, checkpoint\n",
    "\n",
    "import confirm.mini_imprint.lewis_drivers as lts\n",
    "\n",
    "from rich import print as rprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration used during simulation\n",
    "name = \"4d_full\"\n",
    "params = {\n",
    "    \"n_arms\": 4,\n",
    "    \"n_stage_1\": 50,\n",
    "    \"n_stage_2\": 100,\n",
    "    \"n_stage_1_interims\": 2,\n",
    "    \"n_stage_1_add_per_interim\": 100,\n",
    "    \"n_stage_2_add_per_interim\": 100,\n",
    "    \"stage_1_futility_threshold\": 0.15,\n",
    "    \"stage_1_efficacy_threshold\": 0.7,\n",
    "    \"stage_2_futility_threshold\": 0.2,\n",
    "    \"stage_2_efficacy_threshold\": 0.95,\n",
    "    \"inter_stage_futility_threshold\": 0.6,\n",
    "    \"posterior_difference_threshold\": 0,\n",
    "    \"rejection_threshold\": 0.05,\n",
    "    \"key\": jax.random.PRNGKey(0),\n",
    "    \"n_table_pts\": 20,\n",
    "    \"n_pr_sims\": 100,\n",
    "    \"n_sig2_sims\": 20,\n",
    "    \"batch_size\": int(2**12),\n",
    "    \"cache_tables\": f\"./{name}/lei_cache.pkl\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration used during simulation\n",
    "# name = \"3d_smaller2\"\n",
    "# params = {\n",
    "#     \"n_arms\": 3,\n",
    "#     \"n_stage_1\": 50,\n",
    "#     \"n_stage_2\": 100,\n",
    "#     \"n_stage_1_interims\": 2,\n",
    "#     \"n_stage_1_add_per_interim\": 100,\n",
    "#     \"n_stage_2_add_per_interim\": 100,\n",
    "#     \"stage_1_futility_threshold\": 0.15,\n",
    "#     \"stage_1_efficacy_threshold\": 0.7,\n",
    "#     \"stage_2_futility_threshold\": 0.2,\n",
    "#     \"stage_2_efficacy_threshold\": 0.95,\n",
    "#     \"inter_stage_futility_threshold\": 0.6,\n",
    "#     \"posterior_difference_threshold\": 0,\n",
    "#     \"rejection_threshold\": 0.05,\n",
    "#     \"key\": jax.random.PRNGKey(0),\n",
    "#     \"n_table_pts\": 20,\n",
    "#     \"n_pr_sims\": 100,\n",
    "#     \"n_sig2_sims\": 20,\n",
    "#     \"batch_size\": int(2**12),\n",
    "#     \"cache_tables\": f\"./{name}/lei_cache.pkl\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_obj = lewis.Lewis45(**params)\n",
    "n_arm_samples = int(lei_obj.unifs_shape()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = params[\"n_arms\"]\n",
    "ns = np.concatenate(\n",
    "    [np.ones(n_arms - 1)[:, None], -np.eye(n_arms - 1)],\n",
    "    axis=-1,\n",
    ")\n",
    "null_hypos = [grid.HyperPlane(n, 0) for n in ns]\n",
    "symmetry = []\n",
    "for i in range(n_arms - 2):\n",
    "    n = np.zeros(n_arms)\n",
    "    n[i + 1] = 1\n",
    "    n[i + 2] = -1\n",
    "    symmetry.append(grid.HyperPlane(n, 0))\n",
    "\n",
    "theta_min = -1.0\n",
    "theta_max = 1.0\n",
    "init_grid_size = 8\n",
    "theta, radii = grid.cartesian_gridpts(\n",
    "    np.full(n_arms, theta_min),\n",
    "    np.full(n_arms, theta_max),\n",
    "    np.full(n_arms, init_grid_size),\n",
    ")\n",
    "g_raw = grid.build_grid(theta, radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_grid_cost = 0.002\n",
    "target_sim_cost = 0.002\n",
    "target_alpha = 0.025\n",
    "holderq = 6\n",
    "\n",
    "grid_batch_size = 2**6 if jax.devices()[0].device_kind == \"cpu\" else 2**10\n",
    "init_nsims = 1000\n",
    "max_sim_double = 8\n",
    "max_sim_size = init_nsims * 2 ** max_sim_double\n",
    "seed = 0\n",
    "src_key = jax.random.PRNGKey(seed)\n",
    "key1, key2, key3 = jax.random.split(src_key, 3)\n",
    "\n",
    "unifs = jax.random.uniform(key=key1, shape=(max_sim_size,) + lei_obj.unifs_shape(), dtype=jnp.float32)\n",
    "unifs_order = jnp.arange(0, unifs.shape[1])\n",
    "nB_global = 50\n",
    "nB_tile = 50\n",
    "bootstrap_idxs = {\n",
    "    K: jnp.concatenate((\n",
    "        jnp.arange(K)[None, :],\n",
    "        jax.random.choice(key2, K, shape=(nB_global, K), replace=True),\n",
    "        jax.random.choice(key3, K, shape=(nB_tile, K), replace=True)\n",
    "    )).astype(jnp.int32)\n",
    "    for K in (init_nsims * 2 ** np.arange(0, max_sim_double + 1))\n",
    "}\n",
    "\n",
    "# batched_tune = lts.grouped_by_sim_size(lei_obj, lts.tunev, grid_batch_size)\n",
    "# batched_rej = lts.grouped_by_sim_size(lei_obj, lts.rejv, grid_batch_size)\n",
    "# batched_many_rej = lts.grouped_by_sim_size(lei_obj, lts.rejvv, grid_batch_size)\n",
    "\n",
    "\n",
    "import confirm.mini_imprint.bound.binomial as ehbound\n",
    "bwd_solver = ehbound.BackwardQCPSolver(n=n_arm_samples)\n",
    "def invert_bound(alpha, theta_0, vertices, n):\n",
    "    v = vertices - theta_0\n",
    "    # NOTE: OPTIMIZATION POTENTIAL: if we ever need faster EH bounds, then we\n",
    "    # can only run the optimizer at a single corner. The bound is still valid\n",
    "    # because we're just using a suboptimal q.\n",
    "\n",
    "    q_opt = jax.vmap(bwd_solver.solve, in_axes=(None, 0, None))(\n",
    "        theta_0, v, alpha\n",
    "    )\n",
    "    return jnp.min(jax.vmap(ehbound.q_holder_bound_bwd, in_axes=(0, None, None, 0, None))(\n",
    "        q_opt, n, theta_0, v, alpha\n",
    "    ))\n",
    "batched_invert_bound = batch.batch(\n",
    "    jax.jit(jax.vmap(invert_bound, in_axes=(None, 0, 0, None)), static_argnums=(0, 3)),\n",
    "    5*grid_batch_size,\n",
    "    in_axes=(None, 0, 0, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint 4d_full/1991.pkl\n"
     ]
    }
   ],
   "source": [
    "load_iter = 'latest'\n",
    "# load_iter = -1\n",
    "if load_iter == 'latest':\n",
    "    # find the file with the largest checkpoint index: name/###.pkl \n",
    "    available_iters = [int(fn[:-4]) for fn in os.listdir(name) if re.match(r'[0-9]+.pkl', fn)]\n",
    "    load_iter = -1 if len(available_iters) == 0 else max(available_iters)\n",
    "\n",
    "if load_iter == -1:\n",
    "    g = grid.build_grid(\n",
    "        theta, radii, null_hypos=null_hypos, symmetry_planes=symmetry, should_prune=True\n",
    "    )\n",
    "    sim_sizes = np.full(g.n_tiles, init_nsims)\n",
    "    bootstrap_cvs = np.empty((g.n_tiles, 4 + nB_global), dtype=float)\n",
    "    pointwise_target_alpha = np.empty(g.n_tiles, dtype=float)\n",
    "    todo = np.ones(g.n_tiles, dtype=bool)\n",
    "    # TODO: remove\n",
    "    typeI_sum = None\n",
    "    hob_upper = None\n",
    "else:\n",
    "    fn = f\"{name}/{load_iter}.pkl\"\n",
    "    print(f'loading checkpoint {fn}')\n",
    "    with open(fn, \"rb\") as f:\n",
    "        (\n",
    "            g,\n",
    "            sim_sizes,\n",
    "            bootstrap_cvs,\n",
    "            typeI_sum,\n",
    "            hob_upper,\n",
    "            pointwise_target_alpha,\n",
    "        ) = pickle.load(f)\n",
    "    todo = np.zeros(g.n_tiles, dtype=bool)\n",
    "    todo[-1] = True\n",
    "    # keep = np.ones(g.n_tiles, dtype=bool)\n",
    "    # for d in range(3):\n",
    "    #     keep &= (g.theta_tiles[:, d] > -1) & (g.theta_tiles[:, d] < 1)\n",
    "    # g = grid.index_grid(g, keep)\n",
    "    # pointwise_target_alpha = pointwise_target_alpha[keep]\n",
    "    # sim_sizes = sim_sizes[keep]\n",
    "    # bootstrap_cvs = bootstrap_cvs[keep]\n",
    "    # typeI_sum = typeI_sum[keep] if typeI_sum is not None else None\n",
    "    # hob_upper = hob_upper[keep] if hob_upper is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration 1992 with 1 tiles to process\n",
      "runtime prediction: inf\n",
      "inverting the bound took 0.14s\n",
      "tuning for 1000 simulations with 1 tiles and batch size (64, 1000)\n",
      "0.032361507415771484\n",
      "0.17927765846252441\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 53 is out of bounds for axis 1 with size 53",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m bootstrap_cvs[todo, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m nB_global] \u001b[39m=\u001b[39m bootstrap_cvs_todo[:, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m nB_global :]\u001b[39m.\u001b[39mmin(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m bootstrap_cvs[todo, \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m nB_global] \u001b[39m=\u001b[39m bootstrap_cvs_todo[:, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m nB_global :]\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m bootstrap_cvs[todo, \u001b[39m3\u001b[39;49m \u001b[39m+\u001b[39;49m nB_global] \u001b[39m=\u001b[39m bootstrap_cvs_todo[:, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m nB_global :]\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m worst_tile \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin(bootstrap_cvs[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m     44\u001b[0m overall_cv \u001b[39m=\u001b[39m bootstrap_cvs[worst_tile, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 53 is out of bounds for axis 1 with size 53"
     ]
    }
   ],
   "source": [
    "ada_step_size = 10 * grid_batch_size\n",
    "ada_min_step_size = grid_batch_size\n",
    "iter_max = 10000\n",
    "cost_per_sim = np.inf\n",
    "for II in range(load_iter + 1, iter_max):\n",
    "    if np.sum(todo) == 0:\n",
    "        break\n",
    "\n",
    "    print(f\"starting iteration {II} with {np.sum(todo)} tiles to process\")\n",
    "    if cost_per_sim is not None:\n",
    "        predicted_time = np.sum(sim_sizes[todo] * cost_per_sim)\n",
    "        print(f\"runtime prediction: {predicted_time:.2f}\")\n",
    "\n",
    "    ########################################\n",
    "    # Simulate any new or updated tiles.\n",
    "    ########################################\n",
    "\n",
    "    start = time.time()\n",
    "    pointwise_target_alpha[todo] = batched_invert_bound(\n",
    "        target_alpha, g.theta_tiles[todo], g.vertices(todo), n_arm_samples\n",
    "    )\n",
    "    print(f\"inverting the bound took {time.time() - start:.2f}s\")\n",
    "    start = time.time()\n",
    "\n",
    "    bootstrap_cvs_todo = lts.bootstrap_tune_runner(\n",
    "        lei_obj,\n",
    "        sim_sizes[todo],\n",
    "        pointwise_target_alpha[todo],\n",
    "        g.theta_tiles[todo],\n",
    "        g.null_truth[todo],\n",
    "        unifs,\n",
    "        bootstrap_idxs,\n",
    "        unifs_order,\n",
    "    )\n",
    "    # TODO: this indexing has been a source of bugs. it would be nice to a\n",
    "    # tile-wise database tool that can give names to different values while\n",
    "    # smoothly handling the refinement, possibly also sparsity?\n",
    "    bootstrap_cvs[todo, 0] = bootstrap_cvs_todo[:, 0]\n",
    "    bootstrap_cvs[todo, 1:1 + nB_global] = bootstrap_cvs_todo[:, 1 : 1 + nB_global]\n",
    "    bootstrap_cvs[todo, 1 + nB_global] = bootstrap_cvs_todo[:, 1 + nB_global :].min(axis=1)\n",
    "    bootstrap_cvs[todo, 2 + nB_global] = bootstrap_cvs_todo[:, 1 + nB_global :].mean(axis=1)\n",
    "    bootstrap_cvs[todo, 3 + nB_global] = bootstrap_cvs_todo[:, 1 + nB_global :].max(axis=1)\n",
    "    worst_tile = np.argmin(bootstrap_cvs[:, 0])\n",
    "    overall_cv = bootstrap_cvs[worst_tile, 0]\n",
    "    cost_per_sim = (time.time() - start) / np.sum(sim_sizes[todo])\n",
    "    todo[:] = False\n",
    "    print(f\"tuning took {time.time() - start:.2f}s\")\n",
    "\n",
    "    ########################################\n",
    "    # Checkpoint\n",
    "    ########################################\n",
    "\n",
    "    start = time.time()\n",
    "    savedata = [g, sim_sizes, bootstrap_cvs, None, None, pointwise_target_alpha]\n",
    "    with open(f\"{name}/{II}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(savedata, f)\n",
    "    for old_II in checkpoint.exponential_delete(II):\n",
    "        fp = f\"{name}/{old_II}.pkl\"\n",
    "        if os.path.exists(fp):\n",
    "            os.remove(fp)\n",
    "    print(f\"checkpointing took {time.time() - start:.2f}s\")\n",
    "\n",
    "    ########################################\n",
    "    # Criterion step 1: is tuning impossible?\n",
    "    ########################################\n",
    "    # try to estimate the number of refinements steps required to get to the\n",
    "    # target alpha. for now, it's okay to slightly preference refinement over\n",
    "    # adding sims because refinment gives more information in a sense.\n",
    "    start = time.time()\n",
    "    cost_to_refine = 2**n_arms\n",
    "    sims_required_to_rej_once = 2 / pointwise_target_alpha - 1\n",
    "    cost_to_rej_once = sims_required_to_rej_once / sim_sizes\n",
    "\n",
    "    # if a tile always stops early, it's probably not interesting and we should\n",
    "    # lean towards simulating more rather than more expensive refinement\n",
    "    always_stops_early = bootstrap_cvs[:, 0] >= 1\n",
    "    prefer_simulation = (cost_to_refine > cost_to_rej_once) & (always_stops_early)\n",
    "\n",
    "    alpha_to_rej_once = 2 / (sim_sizes + 1)\n",
    "    impossible = pointwise_target_alpha < alpha_to_rej_once\n",
    "    impossible_refine = (impossible & (~prefer_simulation)) | (\n",
    "        pointwise_target_alpha == 0\n",
    "    )\n",
    "    impossible_sim = impossible & prefer_simulation\n",
    "\n",
    "    ########################################\n",
    "    # Criterion step 2: what is the bias?\n",
    "    ########################################\n",
    "    bootstrap_min_cvs = np.min(bootstrap_cvs[:, :-3], axis=0)\n",
    "    cv_std = bootstrap_min_cvs.std()\n",
    "    worst_stats = lts.one_stat(\n",
    "        lei_obj,\n",
    "        g.theta_tiles[worst_tile],\n",
    "        g.null_truth[worst_tile],\n",
    "        sim_sizes[worst_tile],\n",
    "        unifs,\n",
    "        unifs_order\n",
    "    )\n",
    "    worst_typeI_sum = (worst_stats[None, :] < bootstrap_min_cvs[:, None]).sum(axis=1)\n",
    "    bias = (worst_typeI_sum[0] - worst_typeI_sum[1:].mean()) / sim_sizes[\n",
    "        worst_tile\n",
    "    ]\n",
    "\n",
    "    ########################################\n",
    "    # Criterion step 3: Refine tiles that are too large, deepen tiles that\n",
    "    # cause too much bias.\n",
    "    ########################################\n",
    "    which_deepen = np.zeros(g.n_tiles, dtype=bool)\n",
    "    which_refine = np.zeros(g.n_tiles, dtype=bool)\n",
    "    alpha_cost = target_alpha - pointwise_target_alpha\n",
    "    twb_min_cv = bootstrap_cvs[:, -3]\n",
    "    twb_mean_cv = bootstrap_cvs[:, -2]\n",
    "    twb_max_cv = bootstrap_cvs[:, -1]\n",
    "\n",
    "    if alpha_cost[worst_tile] > target_grid_cost or bias > target_sim_cost:\n",
    "        sorted_orig_cvs = np.argsort(bootstrap_cvs[:, 0])\n",
    "        dangerous_cv = sorted_orig_cvs[:ada_min_step_size]\n",
    "\n",
    "        inflated_min_cv = twb_min_cv#twb_mean_cv + (twb_min_cv - twb_mean_cv) * 6\n",
    "        sorted_bootstrap_idxs = np.argsort(inflated_min_cv)\n",
    "        dangerous_bootstrap = sorted_bootstrap_idxs[:ada_step_size]\n",
    "        dangerous = np.union1d(dangerous_cv, dangerous_bootstrap)\n",
    "\n",
    "        d_should_refine = alpha_cost[dangerous] > target_grid_cost\n",
    "        deepen_likely_to_work = twb_mean_cv[dangerous] > twb_max_cv[worst_tile]\n",
    "        d_should_deepen = deepen_likely_to_work & (sim_sizes[dangerous] < max_sim_size)\n",
    "        which_refine[dangerous] = d_should_refine & (~d_should_deepen)\n",
    "        which_deepen[dangerous] = d_should_deepen | (bias > target_sim_cost)\n",
    "\n",
    "        which_refine |= impossible_refine\n",
    "        which_deepen |= impossible_sim\n",
    "        which_deepen &= ~which_refine\n",
    "        which_deepen &= sim_sizes < max_sim_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048137"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tilewise_bootstrap_min_cv < 0.044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twb_min_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m twb_min_cv\n",
      "\u001b[0;31mNameError\u001b[0m: name 'twb_min_cv' is not defined"
     ]
    }
   ],
   "source": [
    "twb_min_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    ########################################\n",
    "    # Report current status\n",
    "    ########################################\n",
    "    report = dict(\n",
    "        II=II,\n",
    "        overall_cv=f\"{overall_cv:.5f}\",\n",
    "        cv_std=f\"{cv_std:.4f}\",\n",
    "        grid_cost=f\"{alpha_cost[worst_tile]:.5f}\",\n",
    "        bias=f\"{bias:.5f}\",\n",
    "        n_tiles=g.n_tiles,\n",
    "        n_refine=np.sum(which_refine),\n",
    "        n_refine_impossible=np.sum(impossible_refine),\n",
    "        n_moresims=np.sum(which_deepen),\n",
    "        n_moresims_impossible=np.sum(impossible_sim),\n",
    "        # moresims_dist=np.unique(sim_multiplier, return_counts=True)\n",
    "    )\n",
    "    rprint(report)\n",
    "    print(f\"analysis took\", time.time() - start)\n",
    "    start = time.time()\n",
    "\n",
    "    ########################################\n",
    "    # Refine!\n",
    "    ########################################\n",
    "\n",
    "    if (np.sum(which_refine) > 0 or np.sum(which_deepen) > 0) and II != iter_max - 1:\n",
    "        if np.sum(which_refine) == 0:\n",
    "            sim_sizes[which_deepen] = sim_sizes[which_deepen] * 2\n",
    "            todo[which_deepen] = True\n",
    "\n",
    "        refine_tile_idxs = np.where(which_refine)[0]\n",
    "        refine_gridpt_idxs = g.grid_pt_idx[refine_tile_idxs]\n",
    "        new_thetas, new_radii, keep_tile_idxs = grid.refine_grid(g, refine_gridpt_idxs)\n",
    "        new_grid = grid.build_grid(\n",
    "            new_thetas,\n",
    "            new_radii,\n",
    "            null_hypos=g.null_hypos,\n",
    "            symmetry_planes=symmetry,\n",
    "            should_prune=True,\n",
    "        )\n",
    "\n",
    "        old_g = g\n",
    "        # NOTE: It would be possible to avoid concatenating the grid every\n",
    "        # iteration. For particularly large problems, that might be a large win\n",
    "        # in runtime. But the additional complexity is undesirable at the\n",
    "        # moment. \n",
    "        g = grid.concat_grids(grid.index_grid(old_g, keep_tile_idxs), new_grid)\n",
    "\n",
    "        sim_sizes = np.concatenate(\n",
    "            [sim_sizes[keep_tile_idxs], np.full(new_grid.n_tiles, init_nsims)]\n",
    "        )\n",
    "        todo = np.concatenate(\n",
    "            [todo[keep_tile_idxs], np.ones(new_grid.n_tiles, dtype=bool)]\n",
    "        )\n",
    "        bootstrap_cvs = np.concatenate(\n",
    "            [\n",
    "                bootstrap_cvs[keep_tile_idxs],\n",
    "                np.zeros((new_grid.n_tiles, 4 + nB_global), dtype=float),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        pointwise_target_alpha = np.concatenate(\n",
    "            [\n",
    "                pointwise_target_alpha[keep_tile_idxs],\n",
    "                np.empty(new_grid.n_tiles, dtype=float),\n",
    "            ]\n",
    "        )\n",
    "        print(f\"refinement took {time.time() - start:.2f}s\")\n",
    "        continue\n",
    "    print(\"done!\")\n",
    "    savedata = [g, sim_sizes, bootstrap_cvs, None, None, pointwise_target_alpha]\n",
    "    with open(f\"{name}/{II}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(savedata, f)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedata = [g, sim_sizes, bootstrap_cvs, None, None, pointwise_target_alpha]\n",
    "with open(f\"{name}/{II}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(savedata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for size 1000 with 4721515 tiles took 1573.7245726585388\n"
     ]
    }
   ],
   "source": [
    "typeI_sum = batched_rej(\n",
    "    sim_sizes,\n",
    "    (np.full(sim_sizes.shape[0], overall_cv),\n",
    "    g.theta_tiles,\n",
    "    g.null_truth,),\n",
    "    unifs,\n",
    "    unifs_order,\n",
    ")\n",
    "\n",
    "savedata = [\n",
    "    g,\n",
    "    sim_sizes,\n",
    "    bootstrap_cvs,\n",
    "    typeI_sum,\n",
    "    hob_upper,\n",
    "    pointwise_target_alpha\n",
    "]\n",
    "with open(f\"{name}/final.pkl\", \"wb\") as f:\n",
    "    pickle.dump(savedata, f)\n",
    "\n",
    "# Calculate actual type I errors?\n",
    "typeI_est, typeI_CI = binomial.zero_order_bound(\n",
    "    typeI_sum, sim_sizes, delta_validate, 1.0\n",
    ")\n",
    "typeI_bound = typeI_est + typeI_CI\n",
    "\n",
    "hob_upper = binomial.holder_odi_bound(\n",
    "    typeI_bound, g.theta_tiles, g.vertices, n_arm_samples, holderq\n",
    ")\n",
    "sim_cost = typeI_CI\n",
    "hob_empirical_cost = hob_upper - typeI_bound\n",
    "worst_idx = np.argmax(typeI_est)\n",
    "worst_tile = g.theta_tiles[worst_idx]\n",
    "typeI_est[worst_idx], worst_tile\n",
    "worst_cv_idx = np.argmin(sim_cvs)\n",
    "typeI_est[worst_cv_idx], sim_cvs[worst_cv_idx], g.theta_tiles[worst_cv_idx], pointwise_target_alpha[worst_cv_idx]\n",
    "plt.hist(typeI_est, bins=np.linspace(0.02,0.025, 100))\n",
    "plt.show()\n",
    "\n",
    "theta_0 = np.array([-1.0, -1.0, -1.0])      # sim point\n",
    "v = 0.1 * np.ones(theta_0.shape[0])     # displacement\n",
    "f0 = 0.01                               # Type I Error at theta_0\n",
    "fwd_solver = ehbound.ForwardQCPSolver(n=n_arm_samples)\n",
    "q_opt = fwd_solver.solve(theta_0=theta_0, v=v, a=f0) # optimal q\n",
    "ehbound.q_holder_bound_fwd(q_opt, n_arm_samples, theta_0, v, f0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
